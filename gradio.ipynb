{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install gradio \n",
    "# !pip install vllm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import CTransformers\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.embeddings.base import Embeddings\n",
    "import torch\n",
    "from langchain_community.llms import VLLM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import CTransformers\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "\n",
    "\n",
    "# model_name=\"/home-old/s223795137/models/models--meta-llama--Meta-Llama-3-8B-Instruct/snapshots/e1945c40cd546c78e41f1151f4db032b271faeaa\",  \n",
    "\n",
    "model_name='microsoft/phi-4'\n",
    "\n",
    "# model_name  = 'meta-llama/Llama-3.2-3B-Instruct'\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16)\n",
    "\n",
    "# pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, max_new_tokens=1024,device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 02-25 13:42:08 config.py:510] This model supports multiple tasks: {'classify', 'generate', 'reward', 'score', 'embed'}. Defaulting to 'generate'.\n",
      "INFO 02-25 13:42:08 llm_engine.py:234] Initializing an LLM engine (v0.6.6.post1) with config: model='microsoft/phi-4', speculative_config=None, tokenizer='microsoft/phi-4', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=16384, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=microsoft/phi-4, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\"],\"candidate_compile_sizes\":[],\"compile_sizes\":[],\"capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":256}, use_cached_outputs=False, \n",
      "INFO 02-25 13:42:09 selector.py:120] Using Flash Attention backend.\n",
      "INFO 02-25 13:42:11 model_runner.py:1094] Starting to load model microsoft/phi-4...\n",
      "INFO 02-25 13:42:12 weight_utils.py:251] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73d5bfce7a03469e8bc745772d9f6d95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/6 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 02-25 13:42:22 model_runner.py:1099] Loading model weights took 27.3875 GB\n",
      "INFO 02-25 13:42:24 worker.py:241] Memory profiling takes 1.34 seconds\n",
      "INFO 02-25 13:42:24 worker.py:241] the current vLLM instance can use total_gpu_memory (79.11GiB) x gpu_memory_utilization (0.90) = 71.20GiB\n",
      "INFO 02-25 13:42:24 worker.py:241] model weights take 27.39GiB; non_torch_memory takes 0.17GiB; PyTorch activation peak memory takes 2.14GiB; the rest of the memory reserved for KV Cache is 41.50GiB.\n",
      "INFO 02-25 13:42:24 gpu_executor.py:76] # GPU blocks: 13598, # CPU blocks: 1310\n",
      "INFO 02-25 13:42:24 gpu_executor.py:80] Maximum concurrency for 16384 tokens per request: 13.28x\n",
      "INFO 02-25 13:42:26 model_runner.py:1415] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:14<00:00,  2.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 02-25 13:42:40 model_runner.py:1535] Graph capturing finished in 14 secs, took 1.07 GiB\n",
      "INFO 02-25 13:42:40 llm_engine.py:431] init engine (profile, create kv cache, warmup model) took 18.01 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.llms import VLLM\n",
    "\n",
    "llm = VLLM(\n",
    "    model=model_name,\n",
    "    tensor_parallel_size=1,\n",
    "    trust_remote_code=True,  # mandatory for hf models\n",
    "    max_new_tokens=32000,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# llm = HuggingFacePipeline(pipeline=pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "!!!!!!!!!!!!megablocks not available, using torch.matmul instead\n",
      "<All keys matched successfully>\n"
     ]
    }
   ],
   "source": [
    "class SentenceTransformerEmbeddings(Embeddings):\n",
    "    def __init__(self, model_name=\"nomic-ai/nomic-embed-text-v1.5\", device=None, batch_size=32):\n",
    "        super().__init__()\n",
    "        self.device = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model = SentenceTransformer(model_name, trust_remote_code=True, device=self.device)\n",
    "        self.batch_size = batch_size  # Control batch size\n",
    "\n",
    "    def embed_documents(self, texts):\n",
    "        texts = [\"search_document: \"+i for i in texts]\n",
    "        return self.model.encode(\n",
    "            texts, \n",
    "            convert_to_numpy=True, \n",
    "            device=self.device, \n",
    "            batch_size=self.batch_size\n",
    "        ).tolist()\n",
    "\n",
    "    def embed_query(self, text):\n",
    "        return self.model.encode(\n",
    "            ['search_query: '+text], \n",
    "            convert_to_numpy=True, \n",
    "            device=self.device\n",
    "        )[0].tolist()\n",
    "\n",
    "\n",
    "# Initialize the embedding model\n",
    "local_embeddings = SentenceTransformerEmbeddings(batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_620564/2829555206.py:9: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-chroma package and should be used instead. To use it run `pip install -U :class:`~langchain-chroma` and import as `from :class:`~langchain_chroma import Chroma``.\n",
      "  vectorstore = Chroma(persist_directory=\"./chroma_db_pages_v3_2.5k\", embedding_function=local_embeddings)\n"
     ]
    }
   ],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "# from langchain.retrievers import BaseRetriever\n",
    "\n",
    "# vectorstore = Chroma.from_documents(documents=all_splits, embedding=local_embeddings , persist_directory=\"./chroma_db_pages_v3_2.5k\")\n",
    "\n",
    "# Load your Chroma vector store\n",
    "\n",
    "vectorstore = Chroma(persist_directory=\"./chroma_db_pages_v3_2.5k\", embedding_function=local_embeddings)\n",
    "\n",
    "# Create a retriever from the vector store\n",
    "\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 5})  # Fetch top 5 chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_620564/1921380948.py:16: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
      "  llm_chain = LLMChain(llm=llm, prompt=prompt_template)  # Replace with your LLM\n",
      "/tmp/ipykernel_620564/1921380948.py:19: LangChainDeprecationWarning: This class is deprecated. Use the `create_stuff_documents_chain` constructor instead. See migration guide here: https://python.langchain.com/docs/versions/migrating_chains/stuff_docs_chain/\n",
      "  stuff_chain = StuffDocumentsChain(\n"
     ]
    }
   ],
   "source": [
    "from langchain import LLMChain, PromptTemplate\n",
    "from langchain.chains import RetrievalQA, StuffDocumentsChain\n",
    "\n",
    "# Define your prompt template\n",
    "\n",
    "prompt_template = PromptTemplate(\n",
    "\n",
    "    input_variables=[\"context\", \"question\"],\n",
    "    template=\"\"\"<|im_start|>system<|im_sep|><|im_end|>\n",
    "\n",
    "<|im_start|>user<|im_sep|>Answer the question based on the context below:\\n\\nContext:\\n{context}\\n\\nQuestion:\\n{question}\\n\\nAnswer:<|im_start|>assistant<|im_sep|>\"\"\",\n",
    " \n",
    ")\n",
    "\n",
    "# Define your LLMChain\n",
    "llm_chain = LLMChain(llm=llm, prompt=prompt_template)  # Replace with your LLM\n",
    "\n",
    "\n",
    "stuff_chain = StuffDocumentsChain(\n",
    "    llm_chain=llm_chain,\n",
    "    document_variable_name=\"context\"  # This should match the input variable in your prompt\n",
    ")\n",
    "# Combine into a RetrievalQA chain\n",
    "\n",
    "\n",
    "def extract_answers(answers):\n",
    "\n",
    "\n",
    "    answers = answers.split(\"<|im_start|>assistant<|im_sep|>\")[-1]\n",
    "    # answers = answers.split(\"<|eot_id|>\")[-1]\n",
    "\n",
    "    return answers\n",
    "\n",
    "\n",
    "# Function to query the RAG model\n",
    "# def query_rag_model(query):\n",
    "    \n",
    "#     response = rag_chain.run(query)\n",
    "\n",
    "#     response = extract_answers(response)\n",
    "    \n",
    "#     return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class ReversedStuffDocumentsChain(StuffDocumentsChain):\n",
    "#     def _call(self, inputs: dict) -> dict:\n",
    "#         # Debugging: Check the inputs dictionary structure\n",
    "#         print(f\"Inputs received in _call: {inputs}\")\n",
    "\n",
    "#         # Ensure that the \"input_documents\" key exists in inputs\n",
    "#         if \"input_documents\" not in inputs:\n",
    "#             raise KeyError(\"The key 'input_documents' was not found in inputs\")\n",
    "\n",
    "#         # Get the documents from the inputs (input_documents)\n",
    "#         documents = inputs[\"input_documents\"]\n",
    "        \n",
    "#         # Reverse the order of the documents\n",
    "#         reversed_documents = documents[::-1]\n",
    "        \n",
    "#         # Pass the reversed documents back to the original StuffDocumentsChain logic\n",
    "#         # Convert list of documents into a single string if necessary\n",
    "#         inputs[\"input_documents\"] = reversed_documents\n",
    "        \n",
    "#         # Pass the adjusted inputs to the parent class\n",
    "#         return super()._call(inputs)\n",
    "    \n",
    "    \n",
    "# llm_chain = LLMChain(llm=llm, prompt=prompt_template)  # Replace with your LLM\n",
    "\n",
    "# # Use the custom chain\n",
    "# reversed_stuff_chain = ReversedStuffDocumentsChain(\n",
    "#     llm_chain=llm_chain,\n",
    "#     document_variable_name=\"context\"  # This should match the input variable in your prompt\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Update the retriever's 'k' value\n",
    "# retriever = vectorstore.as_retriever(search_kwargs={\"k\": 5})  # Use the dynamic 'k'\n",
    "\n",
    "# rag_chain_reversed = RetrievalQA(\n",
    "\n",
    "#     retriever=retriever,\n",
    "\n",
    "#     combine_documents_chain=reversed_stuff_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs received in _call: {'input_documents': [Document(metadata={'source': '/weka/s223795137/Crawl_data/pages_datasets/IT Help - Change your password using the Deakin website - httpshelp.deakin.edu.auithelphelp.deakin.edu.auithelpid=kb_article_view&sys_kb_id=8324c103c3420a14d95d79aa050131a6.txt'}, page_content='search_document: IT Help - Change your password using the Deakin website Skip to page content Welcome to Deakin, let\\'s get started with your Digital Essentials! Get help with activating your Deakin IT account,  connecting to wifi (Eduroam), printing on campus, installing Microsoft 365 apps, and accessing software for your units. Click here to visit \"Getting Started with your Digital Essentials\". MORE Toggle navigation Log in Home Knowledge IT Help (Knowledge Base) Access & Passwords IT Help - Change your password using the Deakin website BMP Created with Sketch. BMP ZIP Created with Sketch. ZIP XLS Created with Sketch. XLS TXT Created with Sketch. TXT PPT Created with Sketch. PPT PNG Created with Sketch. PNG PDF Created with Sketch. PD F JPG Created with Sketch. JPG GIF Created with Sketch. GIF DOC Created with Sketch. DOC Error Created with Sketch. KB0010278 Change your password using the Deakin website Article metadata. Revised by IT Service Desk This article was updated • 9mo ago 9 months ago This article has 5398 views. • 5398 Views To reset a forgotten password , please do not use the following instructions, and instead refer to the article below: Usernames & Passwords – Support Homepage If you would like to change your password, and already know your current password: Navigate to the Deakin change password webpage: Change Password Enter your Deakin username and current password . Enter a new password that is between 15 to 30 characters , and is different from your previous 5 passwords . For more information about Deakin\\'s password requirements: What are Deakin\\'s password requirements? Type your new password in the confirm password field as well. Click Change password . Note: Each time you change your Deakin password, you will also need to update the password for the Deakin services you have on your mobile device. If you don\\'t update your details, your account may be locked, as your mobile devices will continually try and use your old password. Please close Outlook prior to changing your password for the same reason. Copy Permalink Most Useful Usernames & Passwords – Support Homepage Article Metadata 46767 Views Article has 46767 views • updated 7d ago 7 days ago • Article has rating - 3 out of 5 stars (*) (*) (*) ( ) ( ) How to activate your Deakin IT account Article Metadata 11866 Views Article has 11866 views • updated 5mo ago 5 months ago • Article has rating - 3 out of 5 stars (*) (*) (*) ( ) ( ) What are Deakin\\'s password requirements? Article'), Document(metadata={'source': '/weka/s223795137/Crawl_data/pages_datasets/IT Help - Change your password using the Deakin website - httpshelp.deakin.edu.auithelpid=kb_article_view&sys_kb_id=8324c103c3420a14d95d79aa050131a6.txt'}, page_content='search_document: IT Help - Change your password using the Deakin website Skip to page content Welcome to Deakin, let\\'s get started with your Digital Essentials! Get help with activating your Deakin IT account,  connecting to wifi (Eduroam), printing on campus, installing Microsoft 365 apps, and accessing software for your units. Click here to visit \"Getting Started with your Digital Essentials\". MORE Toggle navigation Log in Home Knowledge IT Help (Knowledge Base) Access & Passwords IT Help - Change your password using the Deakin website BMP Created with Sketch. BMP ZIP Created with Sketch. ZIP XLS Created with Sketch. XLS TXT Created with Sketch. TXT PPT Created with Sketch. PPT PNG Created with Sketch. PNG PDF Created with Sketch. PD F JPG Created with Sketch. JPG GIF Created with Sketch. GIF DOC Created with Sketch. DOC Error Created with Sketch. KB0010278 Change your password using the Deakin website Article metadata. Revised by IT Service Desk This article was updated • 9mo ago 9 months ago This article has 5394 views. • 5394 Views To reset a forgotten password , please do not use the following instructions, and instead refer to the article below: Usernames & Passwords – Support Homepage If you would like to change your password, and already know your current password: Navigate to the Deakin change password webpage: Change Password Enter your Deakin username and current password . Enter a new password that is between 15 to 30 characters , and is different from your previous 5 passwords . For more information about Deakin\\'s password requirements: What are Deakin\\'s password requirements? Type your new password in the confirm password field as well. Click Change password . Note: Each time you change your Deakin password, you will also need to update the password for the Deakin services you have on your mobile device. If you don\\'t update your details, your account may be locked, as your mobile devices will continually try and use your old password. Please close Outlook prior to changing your password for the same reason. Copy Permalink Most Useful Usernames & Passwords – Support Homepage Article Metadata 46728 Views Article has 46728 views • updated 7d ago 7 days ago • Article has rating - 3 out of 5 stars (*) (*) (*) ( ) ( ) How to activate your Deakin IT account Article Metadata 11847 Views Article has 11847 views • updated 5mo ago 5 months ago • Article has rating - 3 out of 5 stars (*) (*) (*) ( ) ( ) What are Deakin\\'s password requirements? Article'), Document(metadata={'source': '/weka/s223795137/Crawl_data/pages_datasets/IT Help - Change your password using the Deakin website - httpshelp.deakin.edu.auithelphelp.deakin.edu.auhelp.deakin.edu.auithelpid=kb_article_view&sys_kb_id=8324c103c3420a14d95d79aa050131a6.txt'}, page_content='search_document: IT Help - Change your password using the Deakin website Skip to page content Welcome to Deakin, let\\'s get started with your Digital Essentials! Get help with activating your Deakin IT account,  connecting to wifi (Eduroam), printing on campus, installing Microsoft 365 apps, and accessing software for your units. Click here to visit \"Getting Started with your Digital Essentials\". MORE Toggle navigation Log in Home Knowledge IT Help (Knowledge Base) Access & Passwords IT Help - Change your password using the Deakin website BMP Created with Sketch. BMP ZIP Created with Sketch. ZIP XLS Created with Sketch. XLS TXT Created with Sketch. TXT PPT Created with Sketch. PPT PNG Created with Sketch. PNG PDF Created with Sketch. PD F JPG Created with Sketch. JPG GIF Created with Sketch. GIF DOC Created with Sketch. DOC Error Created with Sketch. KB0010278 Change your password using the Deakin website Article metadata. Revised by IT Service Desk This article was updated • 9mo ago 9 months ago This article has 5407 views. • 5407 Views To reset a forgotten password , please do not use the following instructions, and instead refer to the article below: Usernames & Passwords – Support Homepage If you would like to change your password, and already know your current password: Navigate to the Deakin change password webpage: Change Password Enter your Deakin username and current password . Enter a new password that is between 15 to 30 characters , and is different from your previous 5 passwords . For more information about Deakin\\'s password requirements: What are Deakin\\'s password requirements? Type your new password in the confirm password field as well. Click Change password . Note: Each time you change your Deakin password, you will also need to update the password for the Deakin services you have on your mobile device. If you don\\'t update your details, your account may be locked, as your mobile devices will continually try and use your old password. Please close Outlook prior to changing your password for the same reason. Copy Permalink Most Useful Usernames & Passwords – Support Homepage Article Metadata 46788 Views Article has 46788 views • updated 7d ago 7 days ago • Article has rating - 3 out of 5 stars (*) (*) (*) ( ) ( ) How to activate your Deakin IT account Article Metadata 11882 Views Article has 11882 views • updated 5mo ago 5 months ago • Article has rating - 3 out of 5 stars (*) (*) (*) ( ) ( ) What are Deakin\\'s password requirements? Article'), Document(metadata={'source': '/weka/s223795137/Crawl_data/pages_datasets/IT Help - Change your password using the Deakin website - httpshelp.deakin.edu.auithelphelp.deakin.edu.auhelp.deakin.edu.auhelp.deakin.edu.auhelp.deakin.edu.auithelpid=kb_article_view&sys_kb_id=8324c103c3420a14d95d79aa050131a6.txt'}, page_content='search_document: IT Help - Change your password using the Deakin website Skip to page content Welcome to Deakin, let\\'s get started with your Digital Essentials! Get help with activating your Deakin IT account,  connecting to wifi (Eduroam), printing on campus, installing Microsoft 365 apps, and accessing software for your units. Click here to visit \"Getting Started with your Digital Essentials\". MORE Toggle navigation Log in Home Knowledge IT Help (Knowledge Base) Access & Passwords IT Help - Change your password using the Deakin website BMP Created with Sketch. BMP ZIP Created with Sketch. ZIP XLS Created with Sketch. XLS TXT Created with Sketch. TXT PPT Created with Sketch. PPT PNG Created with Sketch. PNG PDF Created with Sketch. PD F JPG Created with Sketch. JPG GIF Created with Sketch. GIF DOC Created with Sketch. DOC Error Created with Sketch. KB0010278 Change your password using the Deakin website Article metadata. Revised by IT Service Desk This article was updated • 9mo ago 9 months ago This article has 5424 views. • 5424 Views To reset a forgotten password , please do not use the following instructions, and instead refer to the article below: Usernames & Passwords – Support Homepage If you would like to change your password, and already know your current password: Navigate to the Deakin change password webpage: Change Password Enter your Deakin username and current password . Enter a new password that is between 15 to 30 characters , and is different from your previous 5 passwords . For more information about Deakin\\'s password requirements: What are Deakin\\'s password requirements? Type your new password in the confirm password field as well. Click Change password . Note: Each time you change your Deakin password, you will also need to update the password for the Deakin services you have on your mobile device. If you don\\'t update your details, your account may be locked, as your mobile devices will continually try and use your old password. Please close Outlook prior to changing your password for the same reason. Copy Permalink Most Useful Usernames & Passwords – Support Homepage Article Metadata 46827 Views Article has 46827 views • updated 7d ago 7 days ago • Article has rating - 3 out of 5 stars (*) (*) (*) ( ) ( ) How to activate your Deakin IT account Article Metadata 11915 Views Article has 11915 views • updated 5mo ago 5 months ago • Article has rating - 3 out of 5 stars (*) (*) (*) ( ) ( ) What are Deakin\\'s password requirements? Article'), Document(metadata={'source': '/weka/s223795137/Crawl_data/pages_datasets/IT Help - Change your password using the Deakin website - httpshelp.deakin.edu.auithelphelp.deakin.edu.auhelp.deakin.edu.auhelp.deakin.edu.auithelpid=kb_article_view&sys_kb_id=8324c103c3420a14d95d79aa050131a6.txt'}, page_content='search_document: IT Help - Change your password using the Deakin website Skip to page content Welcome to Deakin, let\\'s get started with your Digital Essentials! Get help with activating your Deakin IT account,  connecting to wifi (Eduroam), printing on campus, installing Microsoft 365 apps, and accessing software for your units. Click here to visit \"Getting Started with your Digital Essentials\". MORE Toggle navigation Log in Home Knowledge IT Help (Knowledge Base) Access & Passwords IT Help - Change your password using the Deakin website BMP Created with Sketch. BMP ZIP Created with Sketch. ZIP XLS Created with Sketch. XLS TXT Created with Sketch. TXT PPT Created with Sketch. PPT PNG Created with Sketch. PNG PDF Created with Sketch. PD F JPG Created with Sketch. JPG GIF Created with Sketch. GIF DOC Created with Sketch. DOC Error Created with Sketch. KB0010278 Change your password using the Deakin website Article metadata. Revised by IT Service Desk This article was updated • 9mo ago 9 months ago This article has 5415 views. • 5415 Views To reset a forgotten password , please do not use the following instructions, and instead refer to the article below: Usernames & Passwords – Support Homepage If you would like to change your password, and already know your current password: Navigate to the Deakin change password webpage: Change Password Enter your Deakin username and current password . Enter a new password that is between 15 to 30 characters , and is different from your previous 5 passwords . For more information about Deakin\\'s password requirements: What are Deakin\\'s password requirements? Type your new password in the confirm password field as well. Click Change password . Note: Each time you change your Deakin password, you will also need to update the password for the Deakin services you have on your mobile device. If you don\\'t update your details, your account may be locked, as your mobile devices will continually try and use your old password. Please close Outlook prior to changing your password for the same reason. Copy Permalink Most Useful Usernames & Passwords – Support Homepage Article Metadata 46810 Views Article has 46810 views • updated 7d ago 7 days ago • Article has rating - 3 out of 5 stars (*) (*) (*) ( ) ( ) How to activate your Deakin IT account Article Metadata 11899 Views Article has 11899 views • updated 5mo ago 5 months ago • Article has rating - 3 out of 5 stars (*) (*) (*) ( ) ( ) What are Deakin\\'s password requirements? Article')], 'question': 'Suggest me a good password to use in Deakin. I love dogs.'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:03<00:00,  3.47s/it, est. speed input: 780.89 toks/s, output: 75.32 toks/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'When creating a secure password, it\\'s crucial to ensure it adheres to specific guidelines for strength and security. Based on the Deakin guidelines you provided, here are some key points to keep in mind:\\n\\n1. **Length**: The password should be between 15 to 30 characters.\\n2. **Complexity**: It should include a mix of uppercase and lowercase letters, numbers, and symbols.\\n3. **Uniqueness**: The password should be different from your previous five passwords.\\n\\nGiven that you love dogs, here\\'s a strategy to create a good password while including your interest:\\n\\n1. Think of a phrase or saying about dogs that you’d remember easily.\\n2. Incorporate numbers, symbols, and uppercase letters.\\n3. Avoid common words or phrases that could be easily guessed.\\n\\nFor example:\\n- Start with a phrase like \"MyDogLovesToRunAndFetch\"\\n- Add numbers and symbols to enhance security: \"MyD0gL0v3sT0Run&F3tch!\"\\n- Adjust the length to fit between 15 and 30 characters and ensure it meets the complexity requirements.\\n\\nThis approach results in a strong and unique password that balances personal significance with security requirements. Remember not to share your password or write it down in an insecure location.'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# res =rag_chain_reversed.run(\"Suggest me a good password to use in Deakin. I love dogs.\")\n",
    "# res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retriever = vectorstore.as_retriever(search_kwargs={\"k\": 5})  # Use the dynamic 'k'\n",
    "\n",
    "# rag_chain = RetrievalQA(\n",
    "\n",
    "#     retriever=retriever,\n",
    "\n",
    "#     combine_documents_chain=stuff_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_620564/2208830183.py:29: LangChainDeprecationWarning: This class is deprecated. Use the `create_retrieval_chain` constructor instead. See migration guide here: https://python.langchain.com/docs/versions/migrating_chains/retrieval_qa/\n",
      "  rag_chain = RetrievalQA(\n",
      "/tmp/ipykernel_620564/2208830183.py:37: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  response = rag_chain.run(query)\n",
      "Processed prompts: 100%|██████████| 1/1 [00:04<00:00,  4.49s/it, est. speed input: 602.89 toks/s, output: 66.17 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When choosing a password, it's important to prioritize security. Given Deakin's requirement that a password must be between 15 to 30 characters and not match any of the previous five passwords, here is a suggestion:\n",
      "\n",
      "1. **Combine Personal Interests and Random Elements:**\n",
      "   - **Base:** Start with something meaningful or memorable to you. Since you love dogs, use a breed name, dog-related phrase, or a memorable dog’s name.\n",
      "   - **Random Characters:** Add random numbers and special characters to increase complexity and security.\n",
      "\n",
      "2. **Example Password:**\n",
      "   - Let's say your favorite dog breed is \"Labrador,\" and you also love hiking. A possible password could be: `Labr@d0r!L0v3sH!k1ngHills987!`\n",
      "   - **Breakdown:** \n",
      "     - \"Labr@d0r\" incorporates a breed name with a mix of upper and lowercase letters and special characters.\n",
      "     - \"!L0v3sH!k1ngHills987!\" adds more complexity.\n",
      "     - The length is well within Deakin's requirements.\n",
      "\n",
      "3. **Security Tips:**\n",
      "   - Avoid using easily guessed information, such as exact pet names, birthdays, or common phrases.\n",
      "   - Use a phrase or sentence that's meaningful to you but obscure enough that others won't guess.\n",
      "\n",
      "Remember to keep your passwords confidential and change them regularly as per the best practices for online security.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "res =query_rag_model(\"Suggest me a good password to use in Deakin. I love dogs.\",k=5)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7861\n",
      "* Running on public URL: https://9d70abdda4264669b2.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://9d70abdda4264669b2.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:01<00:00,  1.08s/it, est. speed input: 2417.03 toks/s, output: 69.53 toks/s]\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "\n",
    "# Define the function that Gradio will call\n",
    "\n",
    "# def rag_interface(query):\n",
    "#     response = query_rag_model(query)\n",
    "#     return response\n",
    "\n",
    "\n",
    "\n",
    "# iface = gr.Interface(\n",
    "#     fn=rag_interface,  # Function to call\n",
    "#     inputs=\"text\",     # Input type (text box)\n",
    "#     outputs=\"text\",    # Output type (text box)\n",
    "#     title=\"LangChain RAG Model\",  # Title of the interface\n",
    "#     description=\"Enter your query and get answers from the LangChain RAG model.\"\n",
    "# )\n",
    "\n",
    "# # Launch the interface\n",
    "\n",
    "\n",
    "\n",
    "def query_rag_model(query, k):\n",
    "\n",
    "    \n",
    "    # Update the retriever's 'k' value\n",
    "    retriever = vectorstore.as_retriever(search_kwargs={\"k\": k})  # Use the dynamic 'k'\n",
    "\n",
    "    rag_chain = RetrievalQA(\n",
    "\n",
    "        retriever=retriever,\n",
    "\n",
    "        combine_documents_chain=stuff_chain)\n",
    "    \n",
    "    \n",
    "    # Run the RAG pipeline\n",
    "    response = rag_chain.run(query)\n",
    "    \n",
    "    # Extract the answer\n",
    "    response = extract_answers(response)\n",
    "    \n",
    "    return response\n",
    "\n",
    "# Gradio interface with a Slider for 'k'\n",
    "def rag_interface(query, k=5):\n",
    "    response = query_rag_model(query, k)\n",
    "    return response\n",
    "\n",
    "# Set up the Gradio interface with a Slider for 'k'\n",
    "iface = gr.Interface(\n",
    "    fn=rag_interface,   # Function to call\n",
    "    inputs=[            # List of inputs: query and k (slider)\n",
    "        gr.Textbox(label=\"Query\", placeholder=\"Enter your question here...\"),\n",
    "        gr.Slider(minimum=1, maximum=10, step=1, value=5, label=\"Number of Documents to Retrieve (k)\")\n",
    "    ], \n",
    "    outputs=\"text\",     # Output type (text box)\n",
    "    title=\"LangChain RAG Model\",\n",
    "    description=\"Enter your query and adjust 'k' to control the number of documents to retrieve for answering.\"\n",
    ")\n",
    "\n",
    "iface.launch(share=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "* Running on public URL: https://a7c61e2a784f5484ca.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://a7c61e2a784f5484ca.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iface.launch(share=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1565481/3741844905.py:29: LangChainDeprecationWarning: This class is deprecated. Use the `create_retrieval_chain` constructor instead. See migration guide here: https://python.langchain.com/docs/versions/migrating_chains/retrieval_qa/\n",
      "  rag_chain = RetrievalQA(\n",
      "/tmp/ipykernel_1565481/3741844905.py:37: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  response = rag_chain.run(query)\n",
      "Processed prompts: 100%|██████████| 1/1 [00:03<00:00,  3.76s/it, est. speed input: 710.34 toks/s, output: 75.29 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.88 s, sys: 46.2 ms, total: 3.92 s\n",
      "Wall time: 3.96 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"When using your Deakin-managed PC, it is important to comply with university policies related to software installation and usage. While Deakin provides tools like the Software Center on Digital Services-managed computers, installing torrent clients is generally discouraged for the following reasons:\\n\\n1. **Policy and Security Concerns**: Deakin University has specific IT policies to ensure the security and integrity of its network and computers. Torrent clients can pose security risks and may be used for distributing and downloading unauthorized content, which violates university policy and could lead to security breaches.\\n\\n2. **Network Usage**: The university network is optimized for academic and professional use. Torrenting can consume significant bandwidth, affecting the performance of the network for other users.\\n\\n3. **Software Guidelines**: The Software Center and IT guidelines typically focus on providing access to applications that support academic work and university operations. It is unlikely that any illegal or non-academic software, such as most torrent clients, would be approved for installation on university computers.\\n\\n4. **Guidance from IT Service Desk**: For software installation, any requests should be in line with the university's guidelines and approved software list. Unauthorized software installations can result in technical and disciplinary actions.\\n\\nTo ensure compliance with Deakin's IT policies and avoid potential issues, it is advisable to refrain from installing torrent clients on Deakin PCs. If you require additional guidance or have specific needs, contacting the IT Service Desk is recommended.\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:04<00:00,  4.42s/it, est. speed input: 604.98 toks/s, output: 75.68 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:04<00:00,  4.75s/it, est. speed input: 577.34 toks/s, output: 75.07 toks/s]\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "query_rag_model('Can I install a torrent client in my Deakin PC?',5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "# Load all text files from a folder\n",
    "loader = DirectoryLoader(\"/weka/s223795137/Crawl_data/pages_datasets\", glob=\"*.txt\")  \n",
    "documents = loader.load()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "li_len = []\n",
    "for i in range(len(documents)):\n",
    "    li_len.append(len(documents[i].page_content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'li_len' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m plt\u001b[38;5;241m.\u001b[39mhist(\u001b[43mli_len\u001b[49m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'li_len' is not defined"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "plt.hist(li_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: line 1: nividia-smi: command not found\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DRAFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/s223795137/.conda/envs/llava/lib/python3.10/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:13: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 02-25 14:01:09 config.py:510] This model supports multiple tasks: {'score', 'generate', 'classify', 'embed', 'reward'}. Defaulting to 'generate'.\n",
      "INFO 02-25 14:01:09 llm_engine.py:234] Initializing an LLM engine (v0.6.6.post1) with config: model='microsoft/phi-4', speculative_config=None, tokenizer='microsoft/phi-4', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=16384, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=microsoft/phi-4, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\"],\"candidate_compile_sizes\":[],\"compile_sizes\":[],\"capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":256}, use_cached_outputs=False, \n",
      "INFO 02-25 14:01:10 selector.py:120] Using Flash Attention backend.\n",
      "INFO 02-25 14:01:11 model_runner.py:1094] Starting to load model microsoft/phi-4...\n",
      "INFO 02-25 14:01:12 weight_utils.py:251] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a37eaf55ad6f46e4b578d137083989ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/6 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 02-25 14:01:19 model_runner.py:1099] Loading model weights took 27.3875 GB\n",
      "INFO 02-25 14:01:20 worker.py:241] Memory profiling takes 1.33 seconds\n",
      "INFO 02-25 14:01:20 worker.py:241] the current vLLM instance can use total_gpu_memory (79.11GiB) x gpu_memory_utilization (0.90) = 71.20GiB\n",
      "INFO 02-25 14:01:20 worker.py:241] model weights take 27.39GiB; non_torch_memory takes 0.17GiB; PyTorch activation peak memory takes 2.14GiB; the rest of the memory reserved for KV Cache is 41.50GiB.\n",
      "INFO 02-25 14:01:20 gpu_executor.py:76] # GPU blocks: 13598, # CPU blocks: 1310\n",
      "INFO 02-25 14:01:20 gpu_executor.py:80] Maximum concurrency for 16384 tokens per request: 13.28x\n",
      "INFO 02-25 14:01:22 model_runner.py:1415] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:16<00:00,  2.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 02-25 14:01:39 model_runner.py:1535] Graph capturing finished in 17 secs, took 1.07 GiB\n",
      "INFO 02-25 14:01:39 llm_engine.py:431] init engine (profile, create kv cache, warmup model) took 20.65 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "!!!!!!!!!!!!megablocks not available, using torch.matmul instead\n",
      "<All keys matched successfully>\n",
      "/tmp/ipykernel_623241/3218999341.py:73: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-chroma package and should be used instead. To use it run `pip install -U :class:`~langchain-chroma` and import as `from :class:`~langchain_chroma import Chroma``.\n",
      "  vectorstore = Chroma(persist_directory=\"./chroma_db_pages_v3_2.5k\", embedding_function=local_embeddings)\n",
      "/tmp/ipykernel_623241/3218999341.py:94: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
      "  llm_chain = LLMChain(llm=llm, prompt=prompt_template)  # Replace with your LLM\n",
      "/tmp/ipykernel_623241/3218999341.py:97: LangChainDeprecationWarning: This class is deprecated. Use the `create_stuff_documents_chain` constructor instead. See migration guide here: https://python.langchain.com/docs/versions/migrating_chains/stuff_docs_chain/\n",
      "  stuff_chain = StuffDocumentsChain(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "* Running on public URL: https://7b7a86eb284b0681cb.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://7b7a86eb284b0681cb.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_623241/3218999341.py:152: LangChainDeprecationWarning: This class is deprecated. Use the `create_retrieval_chain` constructor instead. See migration guide here: https://python.langchain.com/docs/versions/migrating_chains/retrieval_qa/\n",
      "  rag_chain = RetrievalQA(\n",
      "/tmp/ipykernel_623241/3218999341.py:158: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  response = rag_chain.run(query)\n",
      "Processed prompts: 100%|██████████| 1/1 [00:02<00:00,  2.78s/it, est. speed input: 929.41 toks/s, output: 74.44 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:04<00:00,  4.23s/it, est. speed input: 620.25 toks/s, output: 75.49 toks/s]\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "# from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.embeddings.base import Embeddings\n",
    "import torch\n",
    "from langchain import LLMChain, PromptTemplate\n",
    "from langchain.chains import RetrievalQA, StuffDocumentsChain\n",
    "# from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "from langchain.vectorstores import Chroma\n",
    "import gradio as gr\n",
    "import json\n",
    "import os\n",
    "from langchain_community.llms import VLLM\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# model_name=\"/home-old/s223795137/models/models--meta-llama--Meta-Llama-3-8B-Instruct/snapshots/e1945c40cd546c78e41f1151f4db032b271faeaa\",  \n",
    "\n",
    "model_name='microsoft/phi-4'\n",
    "\n",
    "# model_name  = 'meta-llama/Llama-3.2-3B-Instruct'\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16)\n",
    "\n",
    "# pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, max_new_tokens=32000,device='cuda')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# llm = HuggingFacePipeline(pipeline=pipe)\n",
    "\n",
    "\n",
    "llm = VLLM(\n",
    "    model=model_name,\n",
    "    tensor_parallel_size=1,\n",
    "    trust_remote_code=True,  # mandatory for hf models\n",
    "    max_new_tokens=32000,\n",
    ")\n",
    "\n",
    "\n",
    "class SentenceTransformerEmbeddings(Embeddings):\n",
    "    def __init__(self, model_name=\"nomic-ai/nomic-embed-text-v1.5\", device=None, batch_size=32):\n",
    "        super().__init__()\n",
    "        self.device = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model = SentenceTransformer(model_name, trust_remote_code=True, device=self.device)\n",
    "        self.batch_size = batch_size  # Control batch size\n",
    "\n",
    "    def embed_documents(self, texts):\n",
    "        texts = [\"search_document: \"+i for i in texts]\n",
    "        return self.model.encode(\n",
    "            texts, \n",
    "            convert_to_numpy=True, \n",
    "            device=self.device, \n",
    "            batch_size=self.batch_size\n",
    "        ).tolist()\n",
    "\n",
    "    def embed_query(self, text):\n",
    "        return self.model.encode(\n",
    "            ['search_query: '+text], \n",
    "            convert_to_numpy=True, \n",
    "            device=self.device\n",
    "        )[0].tolist()\n",
    "\n",
    "\n",
    "local_embeddings = SentenceTransformerEmbeddings(batch_size=64)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "vectorstore = Chroma(persist_directory=\"./chroma_db_pages_v3_2.5k\", embedding_function=local_embeddings)\n",
    "\n",
    "\n",
    "# retriever = vectorstore.as_retriever(search_kwargs={\"k\": 5})  # Fetch top 5 chunks\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Define your prompt template\n",
    "\n",
    "prompt_template = PromptTemplate(\n",
    "\n",
    "    input_variables=[\"context\", \"question\"],\n",
    "    template=\"\"\"<|im_start|>system<|im_sep|><|im_end|>\n",
    "\n",
    "<|im_start|>user<|im_sep|>Answer the question based on the context below:\\n\\nContext:\\n{context}\\n\\nQuestion:\\n{question}\\n\\nAnswer:<|im_start|>assistant<|im_sep|>\"\"\",\n",
    " \n",
    ")\n",
    "\n",
    "# Define your LLMChain\n",
    "llm_chain = LLMChain(llm=llm, prompt=prompt_template)  # Replace with your LLM\n",
    "\n",
    "\n",
    "stuff_chain = StuffDocumentsChain(\n",
    "    llm_chain=llm_chain,\n",
    "    document_variable_name=\"context\"  \n",
    ")\n",
    "\n",
    "\n",
    "def extract_answers(answers):\n",
    "\n",
    "\n",
    "    answers = answers.split(\"<|im_start|>assistant<|im_sep|>\")[-1]\n",
    "\n",
    "    return answers\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import datetime\n",
    "\n",
    "chat_history_file = \"chat_history.json\"\n",
    "\n",
    "\n",
    "\n",
    "def log_interaction(query, response, filename=\"chat_history.json\"):\n",
    "    \"\"\"Logs the user query and model response to a JSON file.\"\"\"\n",
    "    \n",
    "    # Create an empty list if file doesn't exist\n",
    "    if not os.path.exists(filename):\n",
    "        history = []\n",
    "    else:\n",
    "        with open(filename, \"r\", encoding=\"utf-8\") as file:\n",
    "            try:\n",
    "                history = json.load(file)  # Load existing history\n",
    "            except json.JSONDecodeError:\n",
    "                history = []  # If file is empty or corrupted, reset it\n",
    "\n",
    "    # New entry\n",
    "    entry = {\n",
    "        \"timestamp\": datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "        \"user_query\": query,\n",
    "        \"model_response\": response\n",
    "    }\n",
    "\n",
    "    # Append new entry and save back to JSON file\n",
    "    history.append(entry)\n",
    "\n",
    "    with open(filename, \"w\", encoding=\"utf-8\") as file:\n",
    "        json.dump(history, file, ensure_ascii=False, indent=4)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def query_rag_model(query, k):\n",
    "    # Update the retriever's 'k' value\n",
    "    retriever = vectorstore.as_retriever(search_kwargs={\"k\": k})\n",
    "\n",
    "    rag_chain = RetrievalQA(\n",
    "        retriever=retriever,\n",
    "        combine_documents_chain=stuff_chain\n",
    "    )\n",
    "    \n",
    "    # Run the RAG pipeline\n",
    "    response = rag_chain.run(query)\n",
    "    \n",
    "    # Extract the answer\n",
    "    response = extract_answers(response)\n",
    "\n",
    "    # Log interaction in JSON format\n",
    "    log_interaction(query, response)\n",
    "\n",
    "    return response\n",
    "\n",
    "\n",
    "# def store_rating(query, response, rating):\n",
    "#     \"\"\"Stores the rating after the response is shown.\"\"\"\n",
    "    \n",
    "#     log_interaction(query, response, rating)\n",
    "#     return \"Your feedback has been recorded! ✅\"\n",
    "\n",
    "\n",
    "# Gradio interface with a Slider for 'k'\n",
    "def rag_interface(query, k=5):\n",
    "    response = query_rag_model(query, k)\n",
    "    return response\n",
    "\n",
    "\n",
    "# with gr.Blocks() as iface:\n",
    "    \n",
    "#     gr.Markdown(\"# 📚 Deakin IT AI Help Desk\")\n",
    "\n",
    "#     query_input = gr.Textbox(label=\"Query\", placeholder=\"Enter your question here...\")\n",
    "#     k_slider = gr.Slider(minimum=1, maximum=10, step=1, value=5, label=\"Number of Documents to Retrieve (k)\")\n",
    "#     submit_btn = gr.Button(\"Get Response\")\n",
    "\n",
    "#     response_output = gr.Textbox(label=\"Model Response\", interactive=False)\n",
    "    \n",
    "#     # Feedback Section (Only appears after response)\n",
    "#     rating_dropdown = gr.Dropdown(choices=[\"N/A\", \"1\", \"2\", \"3\", \"4\", \"5\"], value=\"N/A\", label=\"Rate the Response (Optional)\")\n",
    "#     feedback_btn = gr.Button(\"Submit Rating\")\n",
    "\n",
    "#     feedback_message = gr.Textbox(label=\"Feedback Status\", interactive=False)\n",
    "\n",
    "#     # Step 1: Get Response\n",
    "#     submit_btn.click(fn=query_rag_model, inputs=[query_input, k_slider], outputs=response_output)\n",
    "\n",
    "#     # Step 2: Submit Rating\n",
    "#     feedback_btn.click(fn=store_rating, inputs=[query_input, response_output, rating_dropdown], outputs=feedback_message)\n",
    "\n",
    "\n",
    "# Set up the Gradio interface with a Slider for 'k'\n",
    "iface = gr.Interface(\n",
    "    # gr.Markdown(\"# 📚 Deakin IT AI Help Desk\")\n",
    "    \n",
    "    fn=rag_interface,   # Function to call\n",
    "    inputs=[            # List of inputs: query and k (slider)\n",
    "        gr.Textbox(label=\"Query\", placeholder=\"Enter your question here...\"),\n",
    "        gr.Slider(minimum=1, maximum=25, step=1, value=5, label=\"Number of Documents to Retrieve (k)\"),\n",
    "\n",
    "\n",
    "    ], \n",
    "    outputs=\"text\",     # Output type (text box)\n",
    "    title=\"# 📚 Deakin IT AI Help Desk\",\n",
    "    description=\"Enter your query and adjust 'k' to control the number of documents to retrieve for answering.\"\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "iface.launch(share=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llava",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

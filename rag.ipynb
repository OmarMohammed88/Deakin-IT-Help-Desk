{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install -qU langchain-nomic\n",
    "# %pip install unstructured"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !nomic login 'nk-nzSvRBdUz51LmQltAqVa3HXJ4epsEowE4ba8mbn56Fs'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "# Load all text files from a folder\n",
    "loader = DirectoryLoader(\"/weka/s223795137/Crawl_data/pages_datasets\", glob=\"*.txt\")  \n",
    "documents = loader.load()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "\n",
    "# # Load your text document\n",
    "loader = TextLoader(\"website_text.txt\")\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(len(documents)):\n",
    "\n",
    "#     documents[i].page_content = \"search_document: \" + documents[i].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size= 1200,chunk_overlap=0)\n",
    "\n",
    "all_splits = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/s223795137/.conda/envs/llava/lib/python3.10/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:13: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n",
      "!!!!!!!!!!!!megablocks not available, using torch.matmul instead\n",
      "<All keys matched successfully>\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.embeddings.base import Embeddings\n",
    "import torch\n",
    "\n",
    "class SentenceTransformerEmbeddings(Embeddings):\n",
    "    def __init__(self, model_name=\"nomic-ai/nomic-embed-text-v1.5\", device=None, batch_size=32):\n",
    "        super().__init__()\n",
    "        self.device = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model = SentenceTransformer(model_name, trust_remote_code=True, device=self.device)\n",
    "        self.batch_size = batch_size  # Control batch size\n",
    "\n",
    "    def embed_documents(self, texts):\n",
    "\n",
    "        texts = [\"search_document: \"+i for i in texts]\n",
    "        return self.model.encode(\n",
    "            texts, \n",
    "            convert_to_numpy=True, \n",
    "            device=self.device, \n",
    "            batch_size=self.batch_size\n",
    "        ).tolist()\n",
    "\n",
    "    def embed_query(self, text):\n",
    "        return self.model.encode(\n",
    "            ['search_query: '+text], \n",
    "            convert_to_numpy=True, \n",
    "            device=self.device\n",
    "        )[0].tolist()\n",
    "\n",
    "\n",
    "# Initialize the embedding model\n",
    "local_embeddings = SentenceTransformerEmbeddings(batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_chroma import Chroma\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "\n",
    "# local_embeddings = OllamaEmbeddings(model=\"nomic-embed-text\")\n",
    "\n",
    "# vectorstore = Chroma.from_documents(documents=all_splits, embedding=local_embeddings , persist_directory=\"./chroma_db_pages_veach_page\")\n",
    "# vectorstore = Chroma(persist_directory=\"./chroma_db_pages_v3_2.5k\", embedding_function=local_embeddings)\n",
    "\n",
    "\n",
    "\n",
    "vectorstore = Chroma(persist_directory=\"./chroma_db_pages_v3_2.5k\")\n",
    "# vectorstore = Chroma(persist_directory=\"./chroma_db_pages\", embedding_function=local_embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mSignature:\u001b[0m\n",
      "\u001b[0mvectorstore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_documents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mdocuments\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'List[Document]'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0membedding\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'Optional[Embeddings]'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mids\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'Optional[List[str]]'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mcollection_name\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'str'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'langchain'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mpersist_directory\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'Optional[str]'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mclient_settings\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'Optional[chromadb.config.Settings]'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mclient\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'Optional[chromadb.ClientAPI]'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mcollection_metadata\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'Optional[Dict]'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'Any'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;34m'Chroma'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mSource:\u001b[0m   \n",
      "    \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0mfrom_documents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mcls\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mType\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mChroma\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mdocuments\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mDocument\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0membedding\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mEmbeddings\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mids\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mcollection_name\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_LANGCHAIN_DEFAULT_COLLECTION_NAME\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mpersist_directory\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mclient_settings\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mchromadb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSettings\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mclient\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mchromadb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mClientAPI\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Add this line\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mcollection_metadata\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mDict\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mChroma\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;34m\"\"\"Create a Chroma vectorstore from a list of documents.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        If a persist_directory is specified, the collection will be persisted there.\u001b[0m\n",
      "\u001b[0;34m        Otherwise, the data will be ephemeral in-memory.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        Args:\u001b[0m\n",
      "\u001b[0;34m            collection_name: Name of the collection to create.\u001b[0m\n",
      "\u001b[0;34m            persist_directory: Directory to persist the collection.\u001b[0m\n",
      "\u001b[0;34m            ids : List of document IDs. Defaults to None.\u001b[0m\n",
      "\u001b[0;34m            documents: List of documents to add to the vectorstore.\u001b[0m\n",
      "\u001b[0;34m            embedding: Embedding function. Defaults to None.\u001b[0m\n",
      "\u001b[0;34m            client_settings: Chroma client settings.\u001b[0m\n",
      "\u001b[0;34m            client: Chroma client. Documentation:\u001b[0m\n",
      "\u001b[0;34m                    https://docs.trychroma.com/reference/js-client#class:-chromaclient\u001b[0m\n",
      "\u001b[0;34m            collection_metadata: Collection configurations.\u001b[0m\n",
      "\u001b[0;34m                                                  Defaults to None.\u001b[0m\n",
      "\u001b[0;34m            kwargs: Additional keyword arguments to initialize a Chroma client.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        Returns:\u001b[0m\n",
      "\u001b[0;34m            Chroma: Chroma vectorstore.\u001b[0m\n",
      "\u001b[0;34m        \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mtexts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpage_content\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdocuments\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mmetadatas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetadata\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdocuments\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0mids\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muuid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muuid4\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdocuments\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_texts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mtexts\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtexts\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0membedding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mmetadatas\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmetadatas\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mcollection_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollection_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mpersist_directory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpersist_directory\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mclient_settings\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclient_settings\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mclient\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclient\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mcollection_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollection_metadata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFile:\u001b[0m      /home/s223795137/.conda/envs/llava/lib/python3.10/site-packages/langchain_chroma/vectorstores.py\n",
      "\u001b[0;31mType:\u001b[0m      method"
     ]
    }
   ],
   "source": [
    "??vectorstore.from_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_ollama import ChatOllama\n",
    "\n",
    "# model = ChatOllama(\n",
    "#     model=\"/home-old/s223795137/models/models--meta-llama--Meta-Llama-3-8B-Instruct/snapshots/e1945c40cd546c78e41f1151f4db032b271faeaa\",\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# response_message = model.invoke(\n",
    "#     \"Simulate a rap battle between Stephen Colbert and John Oliver\"\n",
    "# )\n",
    "\n",
    "# print(response_message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c106755249154748a552663eccf7b9d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langchain.llms import CTransformers\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "\n",
    "\n",
    "# model_name=\"/home-old/s223795137/models/models--meta-llama--Meta-Llama-3-8B-Instruct/snapshots/e1945c40cd546c78e41f1151f4db032b271faeaa\",  \n",
    "\n",
    "model_name='microsoft/phi-4'\n",
    "\n",
    "# model_name  = 'meta-llama/Llama-3.2-3B-Instruct'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=\"auto\", device_map=\"auto\")\n",
    "\n",
    "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, max_new_tokens=4000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_513427/2691217058.py:1: LangChainDeprecationWarning: The class `HuggingFacePipeline` was deprecated in LangChain 0.0.37 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFacePipeline``.\n",
      "  llm = HuggingFacePipeline(pipeline=pipe)\n"
     ]
    }
   ],
   "source": [
    "llm = HuggingFacePipeline(pipeline=pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_513427/2435010401.py:32: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
      "  llm_chain = LLMChain(llm=llm, prompt=prompt_template)  # Replace with your LLM\n",
      "/tmp/ipykernel_513427/2435010401.py:35: LangChainDeprecationWarning: This class is deprecated. Use the `create_stuff_documents_chain` constructor instead. See migration guide here: https://python.langchain.com/docs/versions/migrating_chains/stuff_docs_chain/\n",
      "  stuff_chain = StuffDocumentsChain(\n",
      "/tmp/ipykernel_513427/2435010401.py:40: LangChainDeprecationWarning: This class is deprecated. Use the `create_retrieval_chain` constructor instead. See migration guide here: https://python.langchain.com/docs/versions/migrating_chains/retrieval_qa/\n",
      "  rag_chain = RetrievalQA(\n"
     ]
    }
   ],
   "source": [
    "from langchain import LLMChain, PromptTemplate\n",
    "from langchain.chains import RetrievalQA, StuffDocumentsChain\n",
    "\n",
    "\n",
    "RAG_TEMPLATE = \"\"\"<|im_start|>system<|im_sep|>\n",
    "You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. Try to Answer the questions based on Your understanding of the provided context texts , If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\n",
    "\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "<|im_end|>\n",
    "<|im_start|>user<|im_sep|>\n",
    "Answer the following question:\n",
    "\n",
    "{question}\n",
    "\n",
    "<|im_start|>assistant<|im_sep|>\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 5})  # Fetch top 5 chunks\n",
    "\n",
    "# Define your prompt template\n",
    "prompt_template = PromptTemplate(\n",
    "    input_variables=[\"context\", \"question\"],\n",
    "    template=\"\"\"<|im_start|>system<|im_sep|><|im_end|>\n",
    "<|im_start|>user<|im_sep|>Answer the question based on the context below:\\n\\nContext:\\n{context}\\n\\nQuestion:\\n{question}\\n\\nAnswer:<|im_start|>assistant<|im_sep|>\"\"\",\n",
    "    # template = RAG_TEMPLATE\n",
    ")\n",
    "\n",
    "# Define your LLMChain\n",
    "llm_chain = LLMChain(llm=llm, prompt=prompt_template)  # Replace with your LLM\n",
    "\n",
    "\n",
    "stuff_chain = StuffDocumentsChain(\n",
    "    llm_chain=llm_chain,\n",
    "    document_variable_name=\"context\"  # This should match the input variable in your prompt\n",
    ")\n",
    "# Combine into a RetrievalQA chain\n",
    "rag_chain = RetrievalQA(\n",
    "    retriever=retriever,\n",
    "    combine_documents_chain=stuff_chain,\n",
    ")\n",
    "\n",
    "\n",
    "def extract_answers(answers):\n",
    "\n",
    "    answers = answers.split(\"<|im_start|>assistant<|im_sep|>\")[-1]\n",
    "\n",
    "    return answers\n",
    "\n",
    "\n",
    "# Function to query the RAG model\n",
    "def query_rag_model(query):\n",
    "    response = rag_chain.run(query)\n",
    "\n",
    "    response = extract_answers(response)\n",
    "    \n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# question = \"i'm a new student and don't know how to activate my account?\"\n",
    "\n",
    "q2 = \"Which alternate formats are available with Blackboard Ally?\"\n",
    "\n",
    "res =rag_chain.run(q2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system<|im_sep|><|im_end|>\n",
      "<|im_start|>user<|im_sep|>Answer the question based on the context below:\n",
      "\n",
      "Context:\n",
      "IT Help - How do I resolve Duo Prompt display issues related to iOS or macOS content restrictions? Skip to page content Welcome to Deakin, let's get started with your Digital Essentials! Get help with activating your Deakin IT account,  connecting to wifi (Eduroam), printing on campus, installing Microsoft 365 apps, and accessing software for your units. Click here to visit \"Getting Started with your Digital Essentials\". MORE Toggle navigation Log in Home Knowledge IT Help (Knowledge Base) Cyber Security - MFA IT Help - How do I resolve Duo Prompt display issues related to iOS or macOS content restrictions? BMP Created with Sketch. BMP ZIP Created with Sketch. ZIP XLS Created with Sketch. XLS TXT Created with Sketch. TXT PPT Created with Sketch. PPT PNG Created with Sketch. PNG PDF Created with Sketch. PD F JPG Created with Sketch. JPG GIF Created with Sketch. GIF DOC Created with Sketch. DOC Error Created with Sketch. KB1000249 How do I resolve Duo Prompt display issues related to iOS or macOS content restrictions? Article metadata. Revised by IT Service Desk This article was updated • 3mo ago 3 months ago This article has 3502 views. • 3502 Views While attempting to log in to a Duo-protected application on iOS or macOS, the Duo Prompt does not display as expected and the following error appears: \"You cannot browse this page at \"duo.com\" because it is restricted\" In some cases, a gray box will appear instead of the Duo Prompt . Some iOS or macOS devices have configurable content restrictions that can potentially prevent the Duo Prompt from displaying correctly. This issue can also be caused by JavaScript being disabled in the Safari browser settings, or by having content restrictions in an Mobile Device Management on the device. To resolve this issue: Make sure that JavaScript is enabled in Safari on your macOS or iOS device. If you have an Mobile Device Management on the device, such as JAMF , please check to determine if the Mobile Device Management settings could be preventing the Duo Prompt from displaying. Disable content restrictions on the device. The instructions are described below for different versions of iOS or macOS. Disabling content restrictions : iOS 12 or newer: iOS 11 or older: macOS 10.15 (Catalina) or higher: iOS 12 or newer: Navigate to Settings > Screen Time > Content & Privacy Restrictions > Content Restrictions > Web Content. Uncheck Limit Adult Websites to completely disable content restrictions If you do not want to fully disable content restrictions, you can allow duosecurity.com within the Content Restrictions page on the iOS device. This will allow the Duo Prompt to display even if content restrictions are enabled. If you are opening Screen Time for the first time and haven't set up the feature, you will be prompted to set up the phone either for yourself or a child and will have the option to set a passcode. iOS 11 or older: Go to Settings > General > Restrictions > Websites . Uncheck Limit Adult Content t o completely disable content restrictions macOS 10.15 (Catalina) or higher: Open System Preferences Navigate to Screen Time > Preferences > Content & Privacy Set any Web Content restrictions to Unrestricted access . If you do not want to fully disable content restrictions, you can allow duosecurity.com within the Content Restrictions page on the iOS device or to the Allowed Websites Only list on macOS. This will allow the Duo Prompt to display even if content restrictions are enabled. Copy Permalink Most Useful Usernames & Passwords – Support Homepage Article Metadata 46732 Views Article has 46732 views • updated 7d ago 7 days ago • Article has rating - 3 out of 5 stars (*) (*) (*) ( ) ( ) How to activate your Deakin IT account Article Metadata 11849 Views Article has 11849 views • updated 5mo ago 5 months ago • Article has rating - 3 out of 5 stars (*) (*) (*) ( ) ( ) What are Deakin's password requirements? Article Metadata 5082 Views Article has 5082 views • updated about a month ago about a\n",
      "\n",
      "IT Help - How do I resolve Duo Prompt display issues related to iOS or macOS content restrictions? Skip to page content Welcome to Deakin, let's get started with your Digital Essentials! Get help with activating your Deakin IT account,  connecting to wifi (Eduroam), printing on campus, installing Microsoft 365 apps, and accessing software for your units. Click here to visit \"Getting Started with your Digital Essentials\". MORE Toggle navigation Log in Home Knowledge IT Help (Knowledge Base) Cyber Security - MFA IT Help - How do I resolve Duo Prompt display issues related to iOS or macOS content restrictions? BMP Created with Sketch. BMP ZIP Created with Sketch. ZIP XLS Created with Sketch. XLS TXT Created with Sketch. TXT PPT Created with Sketch. PPT PNG Created with Sketch. PNG PDF Created with Sketch. PD F JPG Created with Sketch. JPG GIF Created with Sketch. GIF DOC Created with Sketch. DOC Error Created with Sketch. KB1000249 How do I resolve Duo Prompt display issues related to iOS or macOS content restrictions? Article metadata. Revised by IT Service Desk This article was updated • 3mo ago 3 months ago This article has 3523 views. • 3523 Views While attempting to log in to a Duo-protected application on iOS or macOS, the Duo Prompt does not display as expected and the following error appears: \"You cannot browse this page at \"duo.com\" because it is restricted\" In some cases, a gray box will appear instead of the Duo Prompt . Some iOS or macOS devices have configurable content restrictions that can potentially prevent the Duo Prompt from displaying correctly. This issue can also be caused by JavaScript being disabled in the Safari browser settings, or by having content restrictions in an Mobile Device Management on the device. To resolve this issue: Make sure that JavaScript is enabled in Safari on your macOS or iOS device. If you have an Mobile Device Management on the device, such as JAMF , please check to determine if the Mobile Device Management settings could be preventing the Duo Prompt from displaying. Disable content restrictions on the device. The instructions are described below for different versions of iOS or macOS. Disabling content restrictions : iOS 12 or newer: iOS 11 or older: macOS 10.15 (Catalina) or higher: iOS 12 or newer: Navigate to Settings > Screen Time > Content & Privacy Restrictions > Content Restrictions > Web Content. Uncheck Limit Adult Websites to completely disable content restrictions If you do not want to fully disable content restrictions, you can allow duosecurity.com within the Content Restrictions page on the iOS device. This will allow the Duo Prompt to display even if content restrictions are enabled. If you are opening Screen Time for the first time and haven't set up the feature, you will be prompted to set up the phone either for yourself or a child and will have the option to set a passcode. iOS 11 or older: Go to Settings > General > Restrictions > Websites . Uncheck Limit Adult Content t o completely disable content restrictions macOS 10.15 (Catalina) or higher: Open System Preferences Navigate to Screen Time > Preferences > Content & Privacy Set any Web Content restrictions to Unrestricted access . If you do not want to fully disable content restrictions, you can allow duosecurity.com within the Content Restrictions page on the iOS device or to the Allowed Websites Only list on macOS. This will allow the Duo Prompt to display even if content restrictions are enabled. Copy Permalink Most Useful Usernames & Passwords – Support Homepage Article Metadata 46810 Views Article has 46810 views • updated 7d ago 7 days ago • Article has rating - 3 out of 5 stars (*) (*) (*) ( ) ( ) How to activate your Deakin IT account Article Metadata 11899 Views Article has 11899 views • updated 5mo ago 5 months ago • Article has rating - 3 out of 5 stars (*) (*) (*) ( ) ( ) What are Deakin's password requirements? Article Metadata 5105 Views Article has 5105 views • updated about a month ago about a\n",
      "\n",
      "IT Help - How do I resolve Duo Prompt display issues related to iOS or macOS content restrictions? Skip to page content Welcome to Deakin, let's get started with your Digital Essentials! Get help with activating your Deakin IT account,  connecting to wifi (Eduroam), printing on campus, installing Microsoft 365 apps, and accessing software for your units. Click here to visit \"Getting Started with your Digital Essentials\". MORE Toggle navigation Log in Home Knowledge IT Help (Knowledge Base) Cyber Security - MFA IT Help - How do I resolve Duo Prompt display issues related to iOS or macOS content restrictions? BMP Created with Sketch. BMP ZIP Created with Sketch. ZIP XLS Created with Sketch. XLS TXT Created with Sketch. TXT PPT Created with Sketch. PPT PNG Created with Sketch. PNG PDF Created with Sketch. PD F JPG Created with Sketch. JPG GIF Created with Sketch. GIF DOC Created with Sketch. DOC Error Created with Sketch. KB1000249 How do I resolve Duo Prompt display issues related to iOS or macOS content restrictions? Article metadata. Revised by IT Service Desk This article was updated • 3mo ago 3 months ago This article has 3527 views. • 3527 Views While attempting to log in to a Duo-protected application on iOS or macOS, the Duo Prompt does not display as expected and the following error appears: \"You cannot browse this page at \"duo.com\" because it is restricted\" In some cases, a gray box will appear instead of the Duo Prompt . Some iOS or macOS devices have configurable content restrictions that can potentially prevent the Duo Prompt from displaying correctly. This issue can also be caused by JavaScript being disabled in the Safari browser settings, or by having content restrictions in an Mobile Device Management on the device. To resolve this issue: Make sure that JavaScript is enabled in Safari on your macOS or iOS device. If you have an Mobile Device Management on the device, such as JAMF , please check to determine if the Mobile Device Management settings could be preventing the Duo Prompt from displaying. Disable content restrictions on the device. The instructions are described below for different versions of iOS or macOS. Disabling content restrictions : iOS 12 or newer: iOS 11 or older: macOS 10.15 (Catalina) or higher: iOS 12 or newer: Navigate to Settings > Screen Time > Content & Privacy Restrictions > Content Restrictions > Web Content. Uncheck Limit Adult Websites to completely disable content restrictions If you do not want to fully disable content restrictions, you can allow duosecurity.com within the Content Restrictions page on the iOS device. This will allow the Duo Prompt to display even if content restrictions are enabled. If you are opening Screen Time for the first time and haven't set up the feature, you will be prompted to set up the phone either for yourself or a child and will have the option to set a passcode. iOS 11 or older: Go to Settings > General > Restrictions > Websites . Uncheck Limit Adult Content t o completely disable content restrictions macOS 10.15 (Catalina) or higher: Open System Preferences Navigate to Screen Time > Preferences > Content & Privacy Set any Web Content restrictions to Unrestricted access . If you do not want to fully disable content restrictions, you can allow duosecurity.com within the Content Restrictions page on the iOS device or to the Allowed Websites Only list on macOS. This will allow the Duo Prompt to display even if content restrictions are enabled. Copy Permalink Most Useful Usernames & Passwords – Support Homepage Article Metadata 46810 Views Article has 46810 views • updated 7d ago 7 days ago • Article has rating - 3 out of 5 stars (*) (*) (*) ( ) ( ) How to activate your Deakin IT account Article Metadata 11899 Views Article has 11899 views • updated 5mo ago 5 months ago • Article has rating - 3 out of 5 stars (*) (*) (*) ( ) ( ) What are Deakin's password requirements? Article Metadata 5105 Views Article has 5105 views • updated about a month ago about a\n",
      "\n",
      "IT Help - How do I resolve Duo Prompt display issues related to iOS or macOS content restrictions? Skip to page content Welcome to Deakin, let's get started with your Digital Essentials! Get help with activating your Deakin IT account,  connecting to wifi (Eduroam), printing on campus, installing Microsoft 365 apps, and accessing software for your units. Click here to visit \"Getting Started with your Digital Essentials\". MORE Toggle navigation Log in Home Knowledge IT Help (Knowledge Base) Cyber Security - MFA IT Help - How do I resolve Duo Prompt display issues related to iOS or macOS content restrictions? BMP Created with Sketch. BMP ZIP Created with Sketch. ZIP XLS Created with Sketch. XLS TXT Created with Sketch. TXT PPT Created with Sketch. PPT PNG Created with Sketch. PNG PDF Created with Sketch. PD F JPG Created with Sketch. JPG GIF Created with Sketch. GIF DOC Created with Sketch. DOC Error Created with Sketch. KB1000249 How do I resolve Duo Prompt display issues related to iOS or macOS content restrictions? Article metadata. Revised by IT Service Desk This article was updated • 3mo ago 3 months ago This article has 3511 views. • 3511 Views While attempting to log in to a Duo-protected application on iOS or macOS, the Duo Prompt does not display as expected and the following error appears: \"You cannot browse this page at \"duo.com\" because it is restricted\" In some cases, a gray box will appear instead of the Duo Prompt . Some iOS or macOS devices have configurable content restrictions that can potentially prevent the Duo Prompt from displaying correctly. This issue can also be caused by JavaScript being disabled in the Safari browser settings, or by having content restrictions in an Mobile Device Management on the device. To resolve this issue: Make sure that JavaScript is enabled in Safari on your macOS or iOS device. If you have an Mobile Device Management on the device, such as JAMF , please check to determine if the Mobile Device Management settings could be preventing the Duo Prompt from displaying. Disable content restrictions on the device. The instructions are described below for different versions of iOS or macOS. Disabling content restrictions : iOS 12 or newer: iOS 11 or older: macOS 10.15 (Catalina) or higher: iOS 12 or newer: Navigate to Settings > Screen Time > Content & Privacy Restrictions > Content Restrictions > Web Content. Uncheck Limit Adult Websites to completely disable content restrictions If you do not want to fully disable content restrictions, you can allow duosecurity.com within the Content Restrictions page on the iOS device. This will allow the Duo Prompt to display even if content restrictions are enabled. If you are opening Screen Time for the first time and haven't set up the feature, you will be prompted to set up the phone either for yourself or a child and will have the option to set a passcode. iOS 11 or older: Go to Settings > General > Restrictions > Websites . Uncheck Limit Adult Content t o completely disable content restrictions macOS 10.15 (Catalina) or higher: Open System Preferences Navigate to Screen Time > Preferences > Content & Privacy Set any Web Content restrictions to Unrestricted access . If you do not want to fully disable content restrictions, you can allow duosecurity.com within the Content Restrictions page on the iOS device or to the Allowed Websites Only list on macOS. This will allow the Duo Prompt to display even if content restrictions are enabled. Copy Permalink Most Useful Usernames & Passwords – Support Homepage Article Metadata 46767 Views Article has 46767 views • updated 7d ago 7 days ago • Article has rating - 3 out of 5 stars (*) (*) (*) ( ) ( ) How to activate your Deakin IT account Article Metadata 11866 Views Article has 11866 views • updated 5mo ago 5 months ago • Article has rating - 3 out of 5 stars (*) (*) (*) ( ) ( ) What are Deakin's password requirements? Article Metadata 5090 Views Article has 5090 views • updated about a month ago about a\n",
      "\n",
      "IT Help - How do I resolve Duo Prompt display issues related to iOS or macOS content restrictions? Skip to page content Welcome to Deakin, let's get started with your Digital Essentials! Get help with activating your Deakin IT account,  connecting to wifi (Eduroam), printing on campus, installing Microsoft 365 apps, and accessing software for your units. Click here to visit \"Getting Started with your Digital Essentials\". MORE Toggle navigation Log in Home Knowledge IT Help (Knowledge Base) Cyber Security - MFA IT Help - How do I resolve Duo Prompt display issues related to iOS or macOS content restrictions? BMP Created with Sketch. BMP ZIP Created with Sketch. ZIP XLS Created with Sketch. XLS TXT Created with Sketch. TXT PPT Created with Sketch. PPT PNG Created with Sketch. PNG PDF Created with Sketch. PD F JPG Created with Sketch. JPG GIF Created with Sketch. GIF DOC Created with Sketch. DOC Error Created with Sketch. KB1000249 How do I resolve Duo Prompt display issues related to iOS or macOS content restrictions? Article metadata. Revised by IT Service Desk This article was updated • 3mo ago 3 months ago This article has 3519 views. • 3519 Views While attempting to log in to a Duo-protected application on iOS or macOS, the Duo Prompt does not display as expected and the following error appears: \"You cannot browse this page at \"duo.com\" because it is restricted\" In some cases, a gray box will appear instead of the Duo Prompt . Some iOS or macOS devices have configurable content restrictions that can potentially prevent the Duo Prompt from displaying correctly. This issue can also be caused by JavaScript being disabled in the Safari browser settings, or by having content restrictions in an Mobile Device Management on the device. To resolve this issue: Make sure that JavaScript is enabled in Safari on your macOS or iOS device. If you have an Mobile Device Management on the device, such as JAMF , please check to determine if the Mobile Device Management settings could be preventing the Duo Prompt from displaying. Disable content restrictions on the device. The instructions are described below for different versions of iOS or macOS. Disabling content restrictions : iOS 12 or newer: iOS 11 or older: macOS 10.15 (Catalina) or higher: iOS 12 or newer: Navigate to Settings > Screen Time > Content & Privacy Restrictions > Content Restrictions > Web Content. Uncheck Limit Adult Websites to completely disable content restrictions If you do not want to fully disable content restrictions, you can allow duosecurity.com within the Content Restrictions page on the iOS device. This will allow the Duo Prompt to display even if content restrictions are enabled. If you are opening Screen Time for the first time and haven't set up the feature, you will be prompted to set up the phone either for yourself or a child and will have the option to set a passcode. iOS 11 or older: Go to Settings > General > Restrictions > Websites . Uncheck Limit Adult Content t o completely disable content restrictions macOS 10.15 (Catalina) or higher: Open System Preferences Navigate to Screen Time > Preferences > Content & Privacy Set any Web Content restrictions to Unrestricted access . If you do not want to fully disable content restrictions, you can allow duosecurity.com within the Content Restrictions page on the iOS device or to the Allowed Websites Only list on macOS. This will allow the Duo Prompt to display even if content restrictions are enabled. Copy Permalink Most Useful Usernames & Passwords – Support Homepage Article Metadata 46789 Views Article has 46789 views • updated 7d ago 7 days ago • Article has rating - 3 out of 5 stars (*) (*) (*) ( ) ( ) How to activate your Deakin IT account Article Metadata 11883 Views Article has 11883 views • updated 5mo ago 5 months ago • Article has rating - 3 out of 5 stars (*) (*) (*) ( ) ( ) What are Deakin's password requirements? Article Metadata 5097 Views Article has 5097 views • updated about a month ago about a\n",
      "\n",
      "Question:\n",
      "How do I resolve Duo Prompt display issues related to iOS or macOS content restrictions?\n",
      "\n",
      "Answer:<|im_start|>assistant<|im_sep|>To resolve Duo Prompt display issues related to iOS or macOS content restrictions, follow these steps:\n",
      "\n",
      "1. **Enable JavaScript in Safari:**\n",
      "   - Ensure that JavaScript is enabled in Safari on your macOS or iOS device. This is crucial for the Duo Prompt to function correctly.\n",
      "\n",
      "2. **Check Mobile Device Management (MDM) Settings:**\n",
      "   - If your device is managed by an MDM solution (e.g., JAMF), verify that the settings are not preventing the Duo Prompt from displaying. Adjust the settings if necessary.\n",
      "\n",
      "3. **Disable Content Restrictions:**\n",
      "   - **iOS 12 or newer:**\n",
      "     - Go to **Settings > Screen Time > Content & Privacy Restrictions > Content Restrictions > Web Content**.\n",
      "     - Uncheck **Limit Adult Websites** to completely disable content restrictions.\n",
      "     - Alternatively, allow **duosecurity.com** within the Content Restrictions page to enable the Duo Prompt without fully disabling restrictions.\n",
      "     - If setting up Screen Time for the first time, you may need to set up the phone for yourself or a child and set a passcode.\n",
      "\n",
      "   - **iOS 11 or older:**\n",
      "     - Navigate to **Settings > General > Restrictions > Websites**.\n",
      "     - Uncheck **Limit Adult Content** to completely disable content restrictions.\n",
      "\n",
      "   - **macOS 10.15 (Catalina) or higher:**\n",
      "     - Open **System Preferences**.\n",
      "     - Go to **Screen Time > Preferences > Content & Privacy**.\n",
      "     - Set any Web Content restrictions to **Unrestricted access**.\n",
      "     - Alternatively, allow **duosecurity.com** within the Content Restrictions page or add it to the Allowed Websites Only list to enable the Duo Prompt without fully disabling restrictions.\n",
      "\n",
      "By following these steps, you should be able to resolve the Duo Prompt display issues related to content restrictions on your iOS or macOS device.\n"
     ]
    }
   ],
   "source": [
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_core.output_parsers import StrOutputParser\n",
    "# from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# prompt = ChatPromptTemplate.from_template(\n",
    "#     \"Summarize the main themes in these retrieved docs: {docs}\"\n",
    "# )\n",
    "\n",
    "\n",
    "# # Convert loaded documents into strings by concatenating their content\n",
    "# # and ignoring metadata\n",
    "# def format_docs(docs):\n",
    "#     return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "\n",
    "# chain = {\"docs\": format_docs} | prompt | llm | StrOutputParser()\n",
    "\n",
    "# question = \"i'm a new student and don't know how to activate my account?\"\n",
    "\n",
    "# docs = vectorstore.similarity_search(question)\n",
    "\n",
    "# # print(chain.invoke(docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "df =pd.read_csv(\"/weka/s223795137/Crawl_data/chat_history.txt\", sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time: 2025-02-20 11:00:50</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>User: How to run an even in a teaching space i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Model: To run an event in a teaching space at ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1. **Understand the Services Offered:**</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>- Digital Services manages a standard opera...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2. **Room Setups and Equipment:**</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>Model: To make the Duo app remember your crede...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>1. **Sign in using Deakin's Single Sign-On (SS...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>2. **Select 'Remember me':** Before approving ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>3. **Approve the Push Notification:** After se...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>**Note:** This option will not work on compute...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>70 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                            Time: 2025-02-20 11:00:50\n",
       "0   User: How to run an even in a teaching space i...\n",
       "1   Model: To run an event in a teaching space at ...\n",
       "2             1. **Understand the Services Offered:**\n",
       "3      - Digital Services manages a standard opera...\n",
       "4                   2. **Room Setups and Equipment:**\n",
       "..                                                ...\n",
       "65  Model: To make the Duo app remember your crede...\n",
       "66  1. **Sign in using Deakin's Single Sign-On (SS...\n",
       "67  2. **Select 'Remember me':** Before approving ...\n",
       "68  3. **Approve the Push Notification:** After se...\n",
       "69  **Note:** This option will not work on compute...\n",
       "\n",
       "[70 rows x 1 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Page Crawled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "\n",
    "\n",
    "# Load all text files from a folder\n",
    "# loader = DirectoryLoader(\"/weka/s223795137/Crawl_data/crawled_pages\", glob=\"*.txt\",show_progress=True)  \n",
    "\n",
    "# documents = loader.load()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text_splitter = RecursiveCharacterTextSplitter(chunk_size= 2000,chunk_overlap=0)\n",
    "\n",
    "# all_splits = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/s223795137/.conda/envs/llava/lib/python3.10/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:13: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n",
      "!!!!!!!!!!!!megablocks not available, using torch.matmul instead\n",
      "<All keys matched successfully>\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.embeddings.base import Embeddings\n",
    "import torch\n",
    "\n",
    "class SentenceTransformerEmbeddings(Embeddings):\n",
    "    def __init__(self, model_name=\"nomic-ai/nomic-embed-text-v1.5\", device=None, batch_size=32):\n",
    "        super().__init__()\n",
    "        self.device = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model = SentenceTransformer(model_name, trust_remote_code=True, device=self.device)\n",
    "        self.batch_size = batch_size  # Control batch size\n",
    "\n",
    "    def embed_documents(self, texts):\n",
    "\n",
    "        texts = [\"search_document: \"+i for i in texts]\n",
    "        return self.model.encode(\n",
    "            texts, \n",
    "            convert_to_numpy=True, \n",
    "            device=self.device, \n",
    "            batch_size=self.batch_size\n",
    "        ).tolist()\n",
    "\n",
    "    def embed_query(self, text):\n",
    "        return self.model.encode(\n",
    "            ['search_query: '+text], \n",
    "            convert_to_numpy=True, \n",
    "            device=self.device\n",
    "        )[0].tolist()\n",
    "\n",
    "\n",
    "# Initialize the embedding model\n",
    "local_embeddings = SentenceTransformerEmbeddings(batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_chroma import Chroma\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain_community.llms import VLLM\n",
    "\n",
    "\n",
    "\n",
    "# vectorstore = Chroma.from_documents(documents=all_splits, embedding=local_embeddings , persist_directory=\"./citaion_chrome_db\")\n",
    "\n",
    "\n",
    "# vectorstore = Chroma(embedding_function=local_embeddings , persist_directory=\"./citaion_chrome_db\")\n",
    "\n",
    "# vectorstore = Chroma(persist_directory=\"./chroma_db_pages_v3_2.5k\", embedding_function=local_embeddings)\n",
    "\n",
    "vectorstore = Chroma(persist_directory=\"./chroma_db_pages_v3_2.5k\", embedding_function=local_embeddings)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 03-07 11:33:10 config.py:510] This model supports multiple tasks: {'classify', 'embed', 'generate', 'reward', 'score'}. Defaulting to 'generate'.\n",
      "INFO 03-07 11:33:10 llm_engine.py:234] Initializing an LLM engine (v0.6.6.post1) with config: model='microsoft/phi-4', speculative_config=None, tokenizer='microsoft/phi-4', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=16384, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=microsoft/phi-4, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\"],\"candidate_compile_sizes\":[],\"compile_sizes\":[],\"capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":256}, use_cached_outputs=False, \n",
      "INFO 03-07 11:33:12 selector.py:120] Using Flash Attention backend.\n",
      "INFO 03-07 11:33:12 model_runner.py:1094] Starting to load model microsoft/phi-4...\n",
      "INFO 03-07 11:33:12 weight_utils.py:251] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d004de46c6d40739145aea3a1378862",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/6 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 03-07 11:33:18 model_runner.py:1099] Loading model weights took 27.3875 GB\n",
      "INFO 03-07 11:33:19 worker.py:241] Memory profiling takes 1.35 seconds\n",
      "INFO 03-07 11:33:19 worker.py:241] the current vLLM instance can use total_gpu_memory (79.11GiB) x gpu_memory_utilization (0.90) = 71.20GiB\n",
      "INFO 03-07 11:33:19 worker.py:241] model weights take 27.39GiB; non_torch_memory takes 0.17GiB; PyTorch activation peak memory takes 2.14GiB; the rest of the memory reserved for KV Cache is 41.50GiB.\n",
      "INFO 03-07 11:33:19 gpu_executor.py:76] # GPU blocks: 13599, # CPU blocks: 1310\n",
      "INFO 03-07 11:33:19 gpu_executor.py:80] Maximum concurrency for 16384 tokens per request: 13.28x\n",
      "INFO 03-07 11:33:22 model_runner.py:1415] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:16<00:00,  2.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 03-07 11:33:38 model_runner.py:1535] Graph capturing finished in 16 secs, took 1.07 GiB\n",
      "INFO 03-07 11:33:38 llm_engine.py:431] init engine (profile, create kv cache, warmup model) took 19.85 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain.llms import HuggingFacePipeline\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "\n",
    "\n",
    "model_name='microsoft/phi-4'\n",
    "\n",
    "# model_name = 'microsoft/Phi-4-multimodal-instruct'\n",
    "\n",
    "\n",
    "llm = VLLM(\n",
    "        model=model_name,\n",
    "        tensor_parallel_size=1,\n",
    "        trust_remote_code=True,  # mandatory for hf models\n",
    "        max_new_tokens=32000,\n",
    "    )\n",
    "\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16 , attn_implementation='flash_attention_2' , trust_remote_code=True)\n",
    "\n",
    "# pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, max_new_tokens=1024,device='cuda')\n",
    "\n",
    "# llm = HuggingFacePipeline(pipeline=pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class ReversedStuffDocumentsChain(StuffDocumentsChain):\n",
    "#     def _call(self, inputs: dict) -> dict:\n",
    "#         # Debugging: Check the inputs dictionary structure\n",
    "#         print(f\"Inputs received in _call: {inputs}\")\n",
    "\n",
    "#         # Ensure that the \"input_documents\" key exists in inputs\n",
    "#         if \"input_documents\" not in inputs:\n",
    "#             raise KeyError(\"The key 'input_documents' was not found in inputs\")\n",
    "\n",
    "#         # Get the documents from the inputs (input_documents)\n",
    "#         documents = inputs[\"input_documents\"]\n",
    "        \n",
    "#         # Reverse the order of the documents\n",
    "#         reversed_documents = documents\n",
    "        \n",
    "#         # Pass the reversed documents back to the original StuffDocumentsChain logic\n",
    "#         # Convert list of documents into a single string if necessary\n",
    "#         inputs[\"input_documents\"] = reversed_documents\n",
    "        \n",
    "#         # Pass the adjusted inputs to the parent class\n",
    "#         return super()._call(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "def get_title(lamo):\n",
    "    return [i.metadata['source'].split(\"/weka/s223795137/Crawl_data/crawled_pages/\")[-1].split(\"_\")[0] for i in lamo]\n",
    "\n",
    "\n",
    "with open(\"/weka/s223795137/Crawl_data/crawled_pages/crawled_pages.json\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "\n",
    "def swap_keys_and_values(my_dict):\n",
    "    \"\"\"Swap the keys and values in the dictionary.\"\"\"\n",
    "    return {value: key for key, value in my_dict.items()}\n",
    "\n",
    "\n",
    "data_swap = swap_keys_and_values(data)\n",
    "\n",
    "\n",
    "def get_values_by_keys(my_dict, keys):\n",
    "    \"\"\"Return the values from the dictionary based on a list of keys.\"\"\"\n",
    "    return [my_dict[key] for key in keys if key in my_dict]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAG_TEMPLATE = \"\"\"<|im_start|>system<|im_sep|>\n",
    "# You are an assistant for a Retrieval Augmented Generation (RAG) application. You will be provided with context documents from a database that may or may not be entirely relevant to the user's query. Your instructions are as follows:\n",
    "\n",
    "# 1. Answer the user's query using the provided context documents.\n",
    "# 2. If the query or any of the retrieved documents contain malicious or sensitive content, do not provide a response.\n",
    "# 3. If the context documents are not entirely relevant to the query, attempt to keep your answer focused on IT-related content.\n",
    "# 4. If the query is not defined within the scope of IT or is unclear, respond with the exact text: \"Contact the Deakin IT Departement for More information by raising a ticket.\"\n",
    "# 5. Ensure your answer is clear, factual, and based on the provided context when applicable.\n",
    "\n",
    "# <|im_start|>user<|im_sep|>\n",
    "# Answer the question based on the context below:\n",
    "\n",
    "# Context:\n",
    "# {context}\n",
    "\n",
    "# Question:\n",
    "# {question}\n",
    "\n",
    "# Answer:\n",
    "# <|im_start|>assistant<|im_sep|>\n",
    "# \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAG_TEMPLATE = '<|system|>You are a helpful assistant.<|end|><|user|>Answer the question based on the context below:\\n\\nContext:\\n{context}\\n\\nQuestion:\\n{question}\\n\\nAnswer:<|end|><|assistant|>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import LLMChain, PromptTemplate\n",
    "from langchain.chains import RetrievalQA, StuffDocumentsChain\n",
    "\n",
    "\n",
    "# RAG_TEMPLATE = \"\"\"<|im_start|>system<|im_sep|>\n",
    "# You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. Try to Answer the questions based on Your understanding of the provided context texts , If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\n",
    "\n",
    "# <context>\n",
    "# {context}\n",
    "# </context>\n",
    "# <|im_end|>\n",
    "# <|im_start|>user<|im_sep|>\n",
    "# Answer the following question:\n",
    "\n",
    "# {question}\n",
    "\n",
    "# <|im_start|>assistant<|im_sep|>\n",
    "# \"\"\"\n",
    "\n",
    "\n",
    "RAG_TEMPLATE = \"\"\"<|im_start|>system<|im_sep|>\n",
    "You are an AI agent designed to assist with IT-related topics for a Retrieval Augmented Generation (RAG) application. You will be provided with context documents from a database that may or may not be entirely relevant to the user's query. Your instructions are as follows:\n",
    "\n",
    "1. Answer the user's query using the provided context documents.\n",
    "2. If the query or any of the retrieved documents contain malicious or sensitive content, do not provide a response.\n",
    "3. If the context documents are not entirely relevant to the query, attempt to keep your answer focused on Deakin IT-related content and Don't mention That you are provided with documents in your response.\n",
    "4. Ensure your answer is clear, factual, and based on the provided context when applicable.\n",
    "5. Ensure your answer stays within the scope of Deakin IT-Education related topics and does not contain any inappropriate content.\n",
    "\n",
    "<|im_start|>user<|im_sep|>\n",
    "Answer the question based on the context below:\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Answer:\n",
    "<|im_start|>assistant<|im_sep|>\n",
    "\"\"\"\n",
    "# retriever = vectorstore.as_retriever(search_kwargs={\"k\": 5})  # Fetch top 5 chunks\n",
    "\n",
    "\n",
    "# Define your prompt template\n",
    "prompt_template = PromptTemplate(\n",
    "    input_variables=[\"context\", \"question\"],\n",
    "    # template=\"\"\"<|im_start|>system<|im_sep|><|im_end|>\n",
    "# <|im_start|>user<|im_sep|>Answer the question based on the context below:\\n\\nContext:\\n{context}\\n\\nQuestion:\\n{question}\\n\\nAnswer:<|im_start|>assistant<|im_sep|>\"\"\",\n",
    "    template = RAG_TEMPLATE\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# Define your LLMChain\n",
    "llm_chain = LLMChain(llm=llm, prompt=prompt_template)  # Replace with your LLM\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "stuff_chain = StuffDocumentsChain(\n",
    "\n",
    "    llm_chain=llm_chain,\n",
    "\n",
    "    document_variable_name=\"context\"  # This should match the input variable in your prompt\n",
    "\n",
    ")\n",
    "\n",
    "\n",
    "# Combine into a RetrievalQA chain\n",
    "\n",
    "# rag_chain = RetrievalQA(\n",
    "\n",
    "#     retriever=retriever,\n",
    "\n",
    "#     combine_documents_chain=stuff_chain,\n",
    "    \n",
    "# )\n",
    "\n",
    "\n",
    "\n",
    "def extract_answers(answers):\n",
    "\n",
    "    # answers = answers.split(\"<|im_start|>assistant<|im_sep|>\")[-1]\n",
    "    answers = answers.split(\"<|assistant|>\")[-1]\n",
    "\n",
    "    return answers\n",
    "\n",
    "\n",
    "\n",
    "# # Function to query the RAG model\n",
    "# def query_rag_model(query):\n",
    "    \n",
    "#     response = rag_chain.run(query)\n",
    "\n",
    "#     citations = rag_chain.retriever.get_relevant_documents(query)\n",
    "#     # response = extract_answers(response)\n",
    "#     citation_links = get_values_by_keys(data_swap,get_title(citations))\n",
    "\n",
    "#     return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "def calculate_query_doc_similarity(query, vectorstore, k=5):\n",
    "    \"\"\"\n",
    "    Calculate cosine similarities between a query and the top k retrieved documents.\n",
    "    \n",
    "    Args:\n",
    "        query (str): The input query string.\n",
    "        vectorstore: The vector store instance (e.g., Chroma).\n",
    "        k (int): Number of top documents to retrieve (default: 3).\n",
    "    \n",
    "    Returns:\n",
    "        dict: Contains doc_similarity_pairs and average_similarity.\n",
    "    \"\"\"\n",
    "    # Initialize retriever\n",
    "    retriever = vectorstore.as_retriever(search_kwargs={\"k\": k})\n",
    "\n",
    "    # Step 1: Get the query embedding\n",
    "    embedding_function = vectorstore.embeddings  # Use the same embedding function\n",
    "    query_embedding = embedding_function.embed_query(query)\n",
    "\n",
    "    # Step 2: Retrieve top k documents\n",
    "    retrieved_docs = retriever.get_relevant_documents(query)\n",
    "\n",
    "    # Step 3: Recompute embeddings for retrieved documents\n",
    "    doc_embeddings = []\n",
    "    for doc in retrieved_docs:\n",
    "        doc_embedding = embedding_function.embed_query(doc.page_content)\n",
    "        doc_embeddings.append(doc_embedding)\n",
    "\n",
    "    query_embedding = np.array(query_embedding).reshape(1, -1)\n",
    "\n",
    "    doc_embeddings = np.array(doc_embeddings)\n",
    "    \n",
    "    similarities = cosine_similarity(query_embedding, doc_embeddings)[0] if doc_embeddings.size > 0 else np.array([])\n",
    "\n",
    "    print(similarities)\n",
    "    average_similarity = np.mean(similarities) if similarities.size > 0 else 0.0\n",
    "\n",
    "\n",
    "    return average_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = 'how to activate my account?'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 5})\n",
    "\n",
    "# Step 1: Get the query embedding\n",
    "embedding_function = vectorstore.embeddings  # Use the same embedding function\n",
    "\n",
    "query_embedding = embedding_function.embed_query(q)\n",
    "\n",
    "# Step 2: Retrieve top k documents\n",
    "retrieved_docs = retriever.get_relevant_documents(q)\n",
    "\n",
    "# # Step 3: Recompute embeddings for retrieved documents\n",
    "# doc_embeddings = []\n",
    "# for doc in retrieved_docs:\n",
    "#     doc_embedding = embedding_function.embed_query(doc.page_content)\n",
    "#     doc_embeddings.append(doc_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever.get_relevant_documents(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_rag_model(query, k):\n",
    "    # Update the retriever's 'k' value\n",
    "    retriever = vectorstore.as_retriever(search_kwargs={\"k\": k})\n",
    "\n",
    "    rag_chain = RetrievalQA(\n",
    "        retriever=retriever,\n",
    "        combine_documents_chain=stuff_chain\n",
    "    )\n",
    "    \n",
    "    # Run the RAG pipeline\n",
    "    response = rag_chain.run(query)\n",
    "    k\n",
    "    # Extract the answer\n",
    "    response = extract_answers(response)\n",
    "\n",
    "    citations = rag_chain.retriever.get_relevant_documents(query)\n",
    "    # response = extract_answers(response)\n",
    "    citation_links = get_values_by_keys(data_swap,get_title(citations))\n",
    "\n",
    "    sim_score = calculate_query_doc_similarity(query , vectorstore , k)\n",
    "    # Log interaction in JSON format\n",
    "\n",
    "    return response ,citation_links , f\"{sim_score:.4f}\"\n",
    "\n",
    "\n",
    "# def store_rating(query, response, rating):\n",
    "#     \"\"\"Stores the rating after the response is shown.\"\"\"\n",
    "    \n",
    "#     log_interaction(query, response, rating)\n",
    "#     return \"Your feedback has been recorded! ✅\"\n",
    "\n",
    "\n",
    "# Gradio interface with a Slider for 'k'\n",
    "def rag_interface(query, k=5):\n",
    "\n",
    "    response,links , score  = query_rag_model(query, k)\n",
    "\n",
    "    links = list(set(links))\n",
    "\n",
    "    links = [\n",
    "            (f\"Document {i+1}\", f\"{links[i]}\") for i in range(len(links))\n",
    "        ]\n",
    "\n",
    "# Create hyperlinks in HTML format\n",
    "    links_markdown = \"\\n\".join([f\"[{title}]({url})\" for title, url in links])\n",
    "# Combine the text and links\n",
    "\n",
    "    if (len(links) == 0 and float(score) < 0.5) or float(score) < 0.5 :\n",
    "        \n",
    "        response = response\n",
    "    else:\n",
    "\n",
    "        response = response+\"\\n\\nReferences:\\n\\n\" + links_markdown\n",
    "\n",
    "    return response , score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1297917/1259401095.py:5: LangChainDeprecationWarning: This class is deprecated. Use the `create_retrieval_chain` constructor instead. See migration guide here: https://python.langchain.com/docs/versions/migrating_chains/retrieval_qa/\n",
      "  rag_chain = RetrievalQA(\n",
      "/tmp/ipykernel_1297917/1259401095.py:11: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  response = rag_chain.run(query)\n",
      "Processed prompts: 100%|██████████| 1/1 [00:02<00:00,  2.45s/it, est. speed input: 83.74 toks/s, output: 77.61 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "To activate your account, follow these general steps:\n",
      "\n",
      "1. **Check Your Email**: After creating your account, you should have received an activation email. Check your inbox (and spam folder) for an email from Deakin IT, which will include an activation link or instructions.\n",
      "\n",
      "2. **Click the Activation Link**: Open the email and click on the provided link to activate your account. Ensure you’re connected to the internet when you do this.\n",
      "\n",
      "3. **Follow the Instructions**: The link should lead you to a page where you may need to confirm your email address or set a password. Complete the necessary steps.\n",
      "\n",
      "4. **Troubleshooting**: If you do not receive the email or the link does not work, look for an option to resend the activation email. If problems persist, contact Deakin IT support for assistance.\n",
      "\n",
      "For specific account activation steps associated with Deakin's IT systems, refer to their official support or IT services page.\n",
      "----------\n",
      "0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "/tmp/ipykernel_1297917/1259401095.py:16: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  citations = rag_chain.retriever.get_relevant_documents(query)\n"
     ]
    }
   ],
   "source": [
    "res,score =  rag_interface('how to activate my account?')\n",
    "\n",
    "print(res)\n",
    "\n",
    "print(\"-\"*10)\n",
    "\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7866\n",
      "* Running on public URL: https://c6c9ad344445a1e167.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://c6c9ad344445a1e167.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:04<00:00,  4.99s/it, est. speed input: 460.90 toks/s, output: 76.35 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:02<00:00,  2.97s/it, est. speed input: 700.72 toks/s, output: 75.43 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:04<00:00,  4.70s/it, est. speed input: 505.72 toks/s, output: 76.17 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:02<00:00,  2.45s/it, est. speed input: 762.76 toks/s, output: 75.25 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.22it/s, est. speed input: 909.18 toks/s, output: 74.64 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:01<00:00,  1.52s/it, est. speed input: 2559.13 toks/s, output: 67.03 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:09<00:00,  9.90s/it, est. speed input: 869.41 toks/s, output: 66.71 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:01<00:00,  1.31s/it, est. speed input: 6081.07 toks/s, output: 50.63 toks/s]\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "\n",
    "iface = gr.Interface(\n",
    "\n",
    "    fn=rag_interface,  # Function to call\n",
    "    \n",
    "    inputs=[  # List of inputs: query and k (slider)\n",
    "        gr.Textbox(label=\"Query\", placeholder=\"Enter your question here...\"),\n",
    "        gr.Slider(minimum=1, maximum=25, step=1, value=5, label=\"Number of Documents to Retrieve (k)\"),\n",
    "    ]\n",
    "    ,\n",
    "    \n",
    "    outputs=[  # Two outputs: response (markdown) and score (text)\n",
    "        \n",
    "        gr.Markdown(label=\"Response\"),\n",
    "\n",
    "        gr.Textbox(label=\"Confidence Score\"),\n",
    "\n",
    "    ],\n",
    "\n",
    "    \n",
    "    title=\"# 📚 Deakin IT AI Help Desk\",\n",
    "    \n",
    "    description=\"Enter your query and adjust 'k' to control the number of documents to retrieve for answering.\"\n",
    "\n",
    ")\n",
    "\n",
    "\n",
    "iface.launch(share=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "q2 = \"My DUO client is not working. What to do?\"\n",
    "# \n",
    "# res =rag_chain.run(q2,metadata={\"input_documents\":documents,'soruce'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_191284/3931470044.py:1: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  lamo = rag_chain.retriever.get_relevant_documents(q2)\n"
     ]
    }
   ],
   "source": [
    "lamo = rag_chain.retriever.get_relevant_documents(q2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## read json file /weka/s223795137/Crawl_data/crawled_pages/crawled_pages.json\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the keys based on the values from the list lamo\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/s223795137/.conda/envs/llava/lib/python3.10/site-packages/langchain/__init__.py:30: UserWarning: Importing LLMChain from langchain root module is no longer supported. Please use langchain.chains.LLMChain instead.\n",
      "  warnings.warn(\n",
      "/home/s223795137/.conda/envs/llava/lib/python3.10/site-packages/langchain/__init__.py:30: UserWarning: Importing PromptTemplate from langchain root module is no longer supported. Please use langchain_core.prompts.PromptTemplate instead.\n",
      "  warnings.warn(\n",
      "/weka/s223795137/Crawl_data/citaion_file.py:8: LangChainDeprecationWarning: Importing Chroma from langchain.vectorstores is deprecated. Please replace deprecated imports:\n",
      "\n",
      ">> from langchain.vectorstores import Chroma\n",
      "\n",
      "with new imports of:\n",
      "\n",
      ">> from langchain_community.vectorstores import Chroma\n",
      "You can use the langchain cli to **automatically** upgrade many imports. Please see documentation here <https://python.langchain.com/docs/versions/v0_2/>\n",
      "  from langchain.vectorstores import Chroma\n",
      "!!!!!!!!!!!!megablocks not available, using torch.matmul instead\n",
      "<All keys matched successfully>\n",
      "INFO 03-07 07:06:01 config.py:510] This model supports multiple tasks: {'score', 'reward', 'generate', 'classify', 'embed'}. Defaulting to 'generate'.\n",
      "INFO 03-07 07:06:01 llm_engine.py:234] Initializing an LLM engine (v0.6.6.post1) with config: model='microsoft/phi-4', speculative_config=None, tokenizer='microsoft/phi-4', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=16384, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=microsoft/phi-4, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\"],\"candidate_compile_sizes\":[],\"compile_sizes\":[],\"capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":256}, use_cached_outputs=False, \n",
      "INFO 03-07 07:06:03 selector.py:120] Using Flash Attention backend.\n",
      "INFO 03-07 07:06:03 model_runner.py:1094] Starting to load model microsoft/phi-4...\n",
      "INFO 03-07 07:06:04 weight_utils.py:251] Using model weights format ['*.safetensors']\n",
      "Loading safetensors checkpoint shards:   0% Completed | 0/6 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  17% Completed | 1/6 [00:00<00:03,  1.33it/s]\n",
      "Loading safetensors checkpoint shards:  33% Completed | 2/6 [00:01<00:03,  1.08it/s]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 3/6 [00:02<00:02,  1.05it/s]\n",
      "Loading safetensors checkpoint shards:  67% Completed | 4/6 [00:03<00:01,  1.06it/s]\n",
      "Loading safetensors checkpoint shards:  83% Completed | 5/6 [00:04<00:00,  1.05it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 6/6 [00:05<00:00,  1.04it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 6/6 [00:05<00:00,  1.06it/s]\n",
      "\n",
      "INFO 03-07 07:06:10 model_runner.py:1099] Loading model weights took 27.3875 GB\n",
      "INFO 03-07 07:06:12 worker.py:241] Memory profiling takes 1.35 seconds\n",
      "INFO 03-07 07:06:12 worker.py:241] the current vLLM instance can use total_gpu_memory (79.11GiB) x gpu_memory_utilization (0.90) = 71.20GiB\n",
      "INFO 03-07 07:06:12 worker.py:241] model weights take 27.39GiB; non_torch_memory takes 0.17GiB; PyTorch activation peak memory takes 2.14GiB; the rest of the memory reserved for KV Cache is 41.50GiB.\n",
      "INFO 03-07 07:06:12 gpu_executor.py:76] # GPU blocks: 13599, # CPU blocks: 1310\n",
      "INFO 03-07 07:06:12 gpu_executor.py:80] Maximum concurrency for 16384 tokens per request: 13.28x\n",
      "INFO 03-07 07:06:14 model_runner.py:1415] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "Capturing CUDA graph shapes: 100%|██████████████| 35/35 [00:16<00:00,  2.16it/s]\n",
      "INFO 03-07 07:06:31 model_runner.py:1535] Graph capturing finished in 16 secs, took 1.07 GiB\n",
      "INFO 03-07 07:06:31 llm_engine.py:431] init engine (profile, create kv cache, warmup model) took 20.51 seconds\n",
      "<All keys matched successfully>\n",
      "/weka/s223795137/Crawl_data/citaion_file.py:240: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-chroma package and should be used instead. To use it run `pip install -U :class:`~langchain-chroma` and import as `from :class:`~langchain_chroma import Chroma``.\n",
      "  vectorstore = Chroma(persist_directory=\"./citaion_chrome_db\", embedding_function=local_embeddings)\n",
      "/weka/s223795137/Crawl_data/citaion_file.py:261: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
      "  llm_chain = LLMChain(llm=llm, prompt=prompt_template)  # Replace with your LLM\n",
      "/weka/s223795137/Crawl_data/citaion_file.py:264: LangChainDeprecationWarning: This class is deprecated. Use the `create_stuff_documents_chain` constructor instead. See migration guide here: https://python.langchain.com/docs/versions/migrating_chains/stuff_docs_chain/\n",
      "  stuff_chain = StuffDocumentsChain(\n",
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "* Running on public URL: https://28426f0d4ef1a4bb68.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n",
      "/weka/s223795137/Crawl_data/citaion_file.py:169: LangChainDeprecationWarning: This class is deprecated. Use the `create_retrieval_chain` constructor instead. See migration guide here: https://python.langchain.com/docs/versions/migrating_chains/retrieval_qa/\n",
      "  rag_chain = RetrievalQA(\n",
      "/weka/s223795137/Crawl_data/citaion_file.py:175: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  response = rag_chain.run(query)\n",
      "Processed prompts: 100%|█| 1/1 [00:04<00:00,  4.50s/it, est. speed input: 366.49\n",
      "/weka/s223795137/Crawl_data/citaion_file.py:180: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  citations = rag_chain.retriever.get_relevant_documents(query)\n",
      "Processed prompts: 100%|█| 1/1 [00:03<00:00,  3.91s/it, est. speed input: 512.90\n"
     ]
    }
   ],
   "source": [
    "!python citaion_file.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Docling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain\n",
      "  Downloading langchain-0.3.20-py3-none-any.whl.metadata (7.7 kB)\n",
      "Collecting langchain-core<1.0.0,>=0.3.41 (from langchain)\n",
      "  Downloading langchain_core-0.3.41-py3-none-any.whl.metadata (5.9 kB)\n",
      "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.6 in /weka/s223795137/.conda/envs/llava/lib/python3.10/site-packages (from langchain) (0.3.6)\n",
      "Requirement already satisfied: langsmith<0.4,>=0.1.17 in /weka/s223795137/.conda/envs/llava/lib/python3.10/site-packages (from langchain) (0.3.8)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /weka/s223795137/.local/lib/python3.10/site-packages (from langchain) (2.10.5)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /weka/s223795137/.conda/envs/llava/lib/python3.10/site-packages (from langchain) (2.0.38)\n",
      "Requirement already satisfied: requests<3,>=2 in /weka/s223795137/.local/lib/python3.10/site-packages (from langchain) (2.32.3)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /weka/s223795137/.conda/envs/llava/lib/python3.10/site-packages (from langchain) (6.0.2)\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /weka/s223795137/.conda/envs/llava/lib/python3.10/site-packages (from langchain) (4.0.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /weka/s223795137/.local/lib/python3.10/site-packages (from langchain-core<1.0.0,>=0.3.41->langchain) (9.0.0)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /weka/s223795137/.conda/envs/llava/lib/python3.10/site-packages (from langchain-core<1.0.0,>=0.3.41->langchain) (1.33)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in /weka/s223795137/.local/lib/python3.10/site-packages (from langchain-core<1.0.0,>=0.3.41->langchain) (24.2)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in /weka/s223795137/.local/lib/python3.10/site-packages (from langchain-core<1.0.0,>=0.3.41->langchain) (4.12.2)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /weka/s223795137/.local/lib/python3.10/site-packages (from langsmith<0.4,>=0.1.17->langchain) (0.27.2)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /weka/s223795137/.conda/envs/llava/lib/python3.10/site-packages (from langsmith<0.4,>=0.1.17->langchain) (3.10.15)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /weka/s223795137/.conda/envs/llava/lib/python3.10/site-packages (from langsmith<0.4,>=0.1.17->langchain) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /weka/s223795137/.local/lib/python3.10/site-packages (from langsmith<0.4,>=0.1.17->langchain) (0.23.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /weka/s223795137/.local/lib/python3.10/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in /weka/s223795137/.local/lib/python3.10/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.27.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /weka/s223795137/.local/lib/python3.10/site-packages (from requests<3,>=2->langchain) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /weka/s223795137/.conda/envs/llava/lib/python3.10/site-packages (from requests<3,>=2->langchain) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /weka/s223795137/.local/lib/python3.10/site-packages (from requests<3,>=2->langchain) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /weka/s223795137/.conda/envs/llava/lib/python3.10/site-packages (from requests<3,>=2->langchain) (2024.12.14)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /weka/s223795137/.conda/envs/llava/lib/python3.10/site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.1)\n",
      "Requirement already satisfied: anyio in /weka/s223795137/.local/lib/python3.10/site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (4.8.0)\n",
      "Requirement already satisfied: httpcore==1.* in /weka/s223795137/.local/lib/python3.10/site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.0.7)\n",
      "Requirement already satisfied: sniffio in /weka/s223795137/.local/lib/python3.10/site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /weka/s223795137/.local/lib/python3.10/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /weka/s223795137/.conda/envs/llava/lib/python3.10/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.41->langchain) (3.0.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /weka/s223795137/.local/lib/python3.10/site-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.2.2)\n",
      "Downloading langchain-0.3.20-py3-none-any.whl (1.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m55.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading langchain_core-0.3.41-py3-none-any.whl (415 kB)\n",
      "Installing collected packages: langchain-core, langchain\n",
      "  Attempting uninstall: langchain-core\n",
      "    Found existing installation: langchain-core 0.3.35\n",
      "    Uninstalling langchain-core-0.3.35:\n",
      "      Successfully uninstalled langchain-core-0.3.35\n",
      "Successfully installed langchain-0.3.20 langchain-core-0.3.41\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "import glob\n",
    "from pathlib import Path\n",
    "# from docling.backend.pypdfium2_backend import PyPdfiumDocumentBackend\n",
    "from docling.datamodel.base_models import InputFormat\n",
    "from docling.document_converter import (\n",
    "    DocumentConverter,\n",
    "    PdfFormatOption,\n",
    "    WordFormatOption,\n",
    ")\n",
    "from docling.pipeline.simple_pipeline import SimplePipeline\n",
    "from docling.pipeline.standard_pdf_pipeline import StandardPdfPipeline\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.embeddings.base import Embeddings\n",
    "import torch\n",
    "from langchain_chroma import Chroma\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# _log = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# directory_path = '/weka/s223795137/Crawl_data/Docling/'\n",
    "# file_patterns = ['*.txt', '*.html', '*.docx', '*.pdf']\n",
    "# output_dir = 'climate'\n",
    "# embedding_batch_size = 64\n",
    "\n",
    "# vector_database = 'climate_db'\n",
    "\n",
    "# # List to store all the files\n",
    "# all_files = []\n",
    "\n",
    "# # Loop through each pattern and retrieve matching files\n",
    "# for pattern in file_patterns:\n",
    "#     files = glob.glob(os.path.join(directory_path, '**', pattern), recursive=True)\n",
    "#     all_files.extend(files)\n",
    "\n",
    "# input_paths = all_files\n",
    "\n",
    "# # Initialize DocumentConverter for supported formats\n",
    "# doc_converter = DocumentConverter(\n",
    "#     allowed_formats=[\n",
    "#         InputFormat.PDF,\n",
    "#         InputFormat.IMAGE,\n",
    "#         InputFormat.DOCX,\n",
    "#         InputFormat.HTML,\n",
    "#         InputFormat.PPTX,\n",
    "#         InputFormat.ASCIIDOC,\n",
    "#         InputFormat.CSV,\n",
    "#         InputFormat.MD,\n",
    "#     ],\n",
    "#     format_options={\n",
    "#         InputFormat.PDF: PdfFormatOption(\n",
    "#             pipeline_cls=StandardPdfPipeline,\n",
    "#         ),\n",
    "#         InputFormat.DOCX: WordFormatOption(\n",
    "#             pipeline_cls=SimplePipeline\n",
    "#         ),\n",
    "#     },\n",
    "# )\n",
    "\n",
    "# # Function to handle text files separately\n",
    "# def process_text_file(file_path: str) -> dict:\n",
    "#     \"\"\"Process a .txt file and return a simple result structure.\"\"\"\n",
    "#     try:\n",
    "#         with open(file_path, 'r', encoding='utf-8') as f:\n",
    "#             content = f.read()\n",
    "#         # Mimic Docling's result structure\n",
    "#         return {\n",
    "#             \"status\": \"success\",\n",
    "#             \"filename\": Path(file_path).name,\n",
    "#             \"document\": content  # Fake method to match Docling's output\n",
    "\n",
    "#         }\n",
    "#     except Exception as e:\n",
    "#         _log.error(f\"Error processing text file {file_path}: {str(e)}\")\n",
    "#         return {\n",
    "#             \"status\": \"failed\",\n",
    "#             \"filename\": Path(file_path).name,\n",
    "#             \"error\": str(e)\n",
    "#         }\n",
    "\n",
    "# # Separate text files from other formats\n",
    "# txt_files = [f for f in input_paths if f.lower().endswith('.txt')]\n",
    "\n",
    "# non_txt_files = [f for f in input_paths if not f.lower().endswith('.txt')]\n",
    "\n",
    "# # Convert non-text files with Docling\n",
    "# conv_results = []\n",
    "# if non_txt_files:\n",
    "#     conv_results.extend(doc_converter.convert_all(non_txt_files))\n",
    "\n",
    "# # Process text files separately\n",
    "# for txt_file in txt_files:\n",
    "#     txt_result = process_text_file(txt_file)\n",
    "#     conv_results.append(txt_result)\n",
    "\n",
    "# def get_all_data(docs, file_name=output_dir):\n",
    "#     out_path = Path(file_name)\n",
    "\n",
    "#     # Check if the Output_documents directory exists, if not, create it\n",
    "#     out_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "#     for doc in docs:\n",
    "#         if isinstance(doc, dict):\n",
    "#             with (out_path / f\"{doc['filename']}.md\").open(\"w\") as fp:\n",
    "#                 fp.write(doc['document'])\n",
    "#         else:\n",
    "#             with (out_path / f\"{doc.input.file.stem}.md\").open(\"w\") as fp:\n",
    "#                 fp.write(doc.document.export_to_markdown())\n",
    "\n",
    "# get_all_data(conv_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loader = DirectoryLoader('/weka/s223795137/Crawl_data/climate/', glob=\"*.md\")\n",
    "\n",
    "# documents = loader.load()\n",
    "\n",
    "# text_splitter = RecursiveCharacterTextSplitter(chunk_size=1200, chunk_overlap=0)\n",
    "\n",
    "# all_splits = text_splitter.split_documents(documents)\n",
    "\n",
    "# class SentenceTransformerEmbeddings(Embeddings):\n",
    "#     def __init__(self, model_name=\"nomic-ai/nomic-embed-text-v1.5\", device=None, batch_size=32):\n",
    "#         super().__init__()\n",
    "#         self.device = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#         self.model = SentenceTransformer(model_name, trust_remote_code=True, device=self.device)\n",
    "#         self.batch_size = batch_size  # Control batch size\n",
    "\n",
    "#     def embed_documents(self, texts):\n",
    "#         texts = [\"search_document: \"+i for i in texts]\n",
    "#         return self.model.encode(\n",
    "#             texts, \n",
    "#             convert_to_numpy=True, \n",
    "#             device=self.device, \n",
    "#             batch_size=self.batch_size\n",
    "#         ).tolist()\n",
    "\n",
    "#     def embed_query(self, text):\n",
    "#         return self.model.encode(\n",
    "#             ['search_query: '+text], \n",
    "#             convert_to_numpy=True, \n",
    "#             device=self.device\n",
    "#         )[0].tolist()\n",
    "\n",
    "# # Initialize the embedding model\n",
    "# local_embeddings = SentenceTransformerEmbeddings(batch_size=64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorstore = Chroma.from_documents(documents=all_splits, embedding=local_embeddings , persist_directory='climate_db')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python Docling/docling.py --direcotry-path '/weka/s223795137/Crawl_data/Docling' --output_dir 'climate' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/s223795137/.conda/envs/llava/lib/python3.10/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:13: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    }
   ],
   "source": [
    "# from langchain.llms import CTransformers\n",
    "# from langchain.llms import HuggingFacePipeline\n",
    "# from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "from sentence_transformers import SentenceTransformer\n",
    "# from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.embeddings.base import Embeddings\n",
    "import torch\n",
    "from langchain import LLMChain, PromptTemplate\n",
    "from langchain.chains import RetrievalQA, StuffDocumentsChain\n",
    "# from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "from langchain.vectorstores import Chroma\n",
    "import gradio as gr\n",
    "import json\n",
    "import os\n",
    "from langchain_community.llms import VLLM\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# model_name=\"/home-old/s223795137/models/models--meta-llama--Meta-Llama-3-8B-Instruct/snapshots/e1945c40cd546c78e41f1151f4db032b271faeaa\",  \n",
    "\n",
    "model_name='microsoft/phi-4'\n",
    "\n",
    "# model_name  = 'meta-llama/Llama-3.2-3B-Instruct'\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16)\n",
    "\n",
    "# pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, max_new_tokens=32000,device='cuda')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# llm = HuggingFacePipeline(pipeline=pipe)\n",
    "\n",
    "\n",
    "\n",
    "class SentenceTransformerEmbeddings(Embeddings):\n",
    "    def __init__(self, model_name=\"nomic-ai/nomic-embed-text-v1.5\", device=None, batch_size=32):\n",
    "        super().__init__()\n",
    "        self.device = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model = SentenceTransformer(model_name, trust_remote_code=True, device=self.device)\n",
    "        self.batch_size = batch_size  # Control batch size\n",
    "\n",
    "    def embed_documents(self, texts):\n",
    "        texts = [\"search_document: \"+i for i in texts]\n",
    "        return self.model.encode(\n",
    "            texts, \n",
    "            convert_to_numpy=True, \n",
    "            device=self.device, \n",
    "            batch_size=self.batch_size\n",
    "        ).tolist()\n",
    "\n",
    "    def embed_query(self, text):\n",
    "        return self.model.encode(\n",
    "            ['search_query: '+text], \n",
    "            convert_to_numpy=True, \n",
    "            device=self.device\n",
    "        )[0].tolist()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def extract_answers(answers):\n",
    "\n",
    "\n",
    "    answers = answers.split(\"<|im_start|>assistant<|im_sep|>\")[-1]\n",
    "\n",
    "    return answers\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import datetime\n",
    "\n",
    "chat_history_file = \"Climate-related Disclosures.json\"\n",
    "\n",
    "\n",
    "\n",
    "def log_interaction(query, response, filename=\"chat_history.json\"):\n",
    "    \"\"\"Logs the user query and model response to a JSON file.\"\"\"\n",
    "    \n",
    "    # Create an empty list if file doesn't exist\n",
    "    if not os.path.exists(filename):\n",
    "        history = []\n",
    "    else:\n",
    "        with open(filename, \"r\", encoding=\"utf-8\") as file:\n",
    "            try:\n",
    "                history = json.load(file)  # Load existing history\n",
    "            except json.JSONDecodeError:\n",
    "                history = []  # If file is empty or corrupted, reset it\n",
    "\n",
    "    # New entry\n",
    "    entry = {\n",
    "        \"timestamp\": datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "        \"user_query\": query,\n",
    "        \"model_response\": response\n",
    "    }\n",
    "\n",
    "    # Append new entry and save back to JSON file\n",
    "    history.append(entry)\n",
    "\n",
    "    with open(filename, \"w\", encoding=\"utf-8\") as file:\n",
    "        json.dump(history, file, ensure_ascii=False, indent=4)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def query_rag_model(query, k):\n",
    "    # Update the retriever's 'k' value\n",
    "    retriever = vectorstore.as_retriever(search_kwargs={\"k\": k})\n",
    "\n",
    "    rag_chain = RetrievalQA(\n",
    "        retriever=retriever,\n",
    "        combine_documents_chain=stuff_chain\n",
    "    )\n",
    "    \n",
    "    # Run the RAG pipeline\n",
    "    response = rag_chain.run(query)\n",
    "    \n",
    "    # Extract the answer\n",
    "    # response = extract_answers(response)\n",
    "\n",
    "    # Log interaction in JSON format\n",
    "    log_interaction(query, response ,chat_history_file)\n",
    "\n",
    "    return response\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def rag_interface(query, k=5):\n",
    "    response = query_rag_model(query, k)\n",
    "    return response\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 03-05 06:34:30 config.py:510] This model supports multiple tasks: {'generate', 'reward', 'score', 'embed', 'classify'}. Defaulting to 'generate'.\n",
      "INFO 03-05 06:34:30 llm_engine.py:234] Initializing an LLM engine (v0.6.6.post1) with config: model='microsoft/phi-4', speculative_config=None, tokenizer='microsoft/phi-4', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=16384, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=microsoft/phi-4, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\"],\"candidate_compile_sizes\":[],\"compile_sizes\":[],\"capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":256}, use_cached_outputs=False, \n",
      "INFO 03-05 06:34:31 selector.py:120] Using Flash Attention backend.\n",
      "INFO 03-05 06:34:31 model_runner.py:1094] Starting to load model microsoft/phi-4...\n",
      "INFO 03-05 06:34:32 weight_utils.py:251] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b253e08b906452f94cf34c1899b5805",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/6 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 03-05 06:34:40 model_runner.py:1099] Loading model weights took 27.3875 GB\n",
      "INFO 03-05 06:34:42 worker.py:241] Memory profiling takes 1.33 seconds\n",
      "INFO 03-05 06:34:42 worker.py:241] the current vLLM instance can use total_gpu_memory (79.11GiB) x gpu_memory_utilization (0.90) = 71.20GiB\n",
      "INFO 03-05 06:34:42 worker.py:241] model weights take 27.39GiB; non_torch_memory takes 0.17GiB; PyTorch activation peak memory takes 2.14GiB; the rest of the memory reserved for KV Cache is 41.50GiB.\n",
      "INFO 03-05 06:34:42 gpu_executor.py:76] # GPU blocks: 13598, # CPU blocks: 1310\n",
      "INFO 03-05 06:34:42 gpu_executor.py:80] Maximum concurrency for 16384 tokens per request: 13.28x\n",
      "INFO 03-05 06:34:44 model_runner.py:1415] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:18<00:00,  1.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 03-05 06:35:02 model_runner.py:1535] Graph capturing finished in 19 secs, took 1.07 GiB\n",
      "INFO 03-05 06:35:02 llm_engine.py:431] init engine (profile, create kv cache, warmup model) took 22.13 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "!!!!!!!!!!!!megablocks not available, using torch.matmul instead\n",
      "<All keys matched successfully>\n",
      "/tmp/ipykernel_1134353/3445714857.py:15: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-chroma package and should be used instead. To use it run `pip install -U :class:`~langchain-chroma` and import as `from :class:`~langchain_chroma import Chroma``.\n",
      "  vectorstore = Chroma(persist_directory=\"./climate_db\", embedding_function=local_embeddings)\n",
      "/tmp/ipykernel_1134353/3445714857.py:58: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
      "  llm_chain = LLMChain(llm=llm, prompt=prompt_template)  # Replace with your LLM\n",
      "/tmp/ipykernel_1134353/3445714857.py:61: LangChainDeprecationWarning: This class is deprecated. Use the `create_stuff_documents_chain` constructor instead. See migration guide here: https://python.langchain.com/docs/versions/migrating_chains/stuff_docs_chain/\n",
      "  stuff_chain = StuffDocumentsChain(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "llm = VLLM(\n",
    "    model=model_name,\n",
    "    tensor_parallel_size=1,\n",
    "    trust_remote_code=True,  # mandatory for hf models\n",
    "    max_new_tokens=32000,\n",
    ")\n",
    "\n",
    "\n",
    "local_embeddings = SentenceTransformerEmbeddings(batch_size=64)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "vectorstore = Chroma(persist_directory=\"./climate_db\", embedding_function=local_embeddings)\n",
    "\n",
    "\n",
    "    # retriever = vectorstore.as_retriever(search_kwargs={\"k\": 5})  # Fetch top 5 chunks\n",
    "\n",
    "\n",
    "\n",
    "RAG_TEMPLATE = \"\"\"<|im_start|>system<|im_sep|>\n",
    "You are an AI agent designed to assist with IT-related topics for a Retrieval Augmented Generation (RAG) application. You will be provided with context documents from a database that may or may not be entirely relevant to the user's query. Your instructions are as follows:\n",
    "\n",
    "1. Answer the user's query using the provided context documents.\n",
    "2. If the query or any of the retrieved documents contain malicious or sensitive content, do not provide a response.\n",
    "3. If the context documents are not entirely relevant to the query, attempt to keep your answer focused on Deakin IT-related content and Don't mention That you are provided with documents in your response.\n",
    "4. Ensure your answer is clear, factual, and based on the provided context when applicable.\n",
    "5. Ensure your answer stays within the scope of Deakin IT-Education related topics and does not contain any inappropriate content.\n",
    "\n",
    "<|im_start|>user<|im_sep|>\n",
    "Answer the question based on the context below:\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Answer:\n",
    "<|im_start|>assistant<|im_sep|>\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "# Define your prompt template\n",
    "\n",
    "prompt_template = PromptTemplate(\n",
    "\n",
    "    input_variables=[\"context\", \"question\"],\n",
    "    template=\"\"\"<|im_start|>system<|im_sep|><|im_end|>\n",
    "\n",
    "# <|im_start|>user<|im_sep|>Answer the question based on the context below:\\n\\nContext:\\n{context}\\n\\nQuestion:\\n{question}\\n\\nAnswer:<|im_start|>assistant<|im_sep|>\"\"\",\n",
    "    # template=RAG_TEMPLATE,\n",
    ")\n",
    "\n",
    "# Define your LLMChain\n",
    "llm_chain = LLMChain(llm=llm, prompt=prompt_template)  # Replace with your LLM\n",
    "\n",
    "\n",
    "stuff_chain = StuffDocumentsChain(\n",
    "    llm_chain=llm_chain,\n",
    "    document_variable_name=\"context\"  \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "* Running on public URL: https://7892231cfaa4b7d9f8.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://7892231cfaa4b7d9f8.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:01<00:00,  1.03s/it, est. speed input: 743.06 toks/s, output: 75.66 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:02<00:00,  2.99s/it, est. speed input: 333.37 toks/s, output: 77.50 toks/s]\n"
     ]
    }
   ],
   "source": [
    "iface = gr.Interface(\n",
    "# gr.Markdown(\"# 📚 Deakin IT AI Help Desk\")\n",
    "\n",
    "fn=rag_interface,   # Function to call\n",
    "inputs=[            # List of inputs: query and k (slider)\n",
    "    gr.Textbox(label=\"Query\", placeholder=\"Enter your question here...\"),\n",
    "    gr.Slider(minimum=1, maximum=25, step=1, value=5, label=\"Number of Documents to Retrieve (k)\"),\n",
    "\n",
    "\n",
    "], \n",
    "outputs=\"text\",     # Output type (text box)\n",
    "title=\"# 📚 Climate-related Disclosures\",\n",
    "description=\"Enter your query and adjust 'k' to control the number of documents to retrieve for answering.\"\n",
    ")\n",
    "\n",
    "\n",
    "iface.launch(share=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mWarning\u001b[0m: Cannot statically find a gradio demo called demo. Reload work may fail.\n",
      "Watching: \u001b[32m'/weka/s223795137/Crawl_data'\u001b[0m \u001b[32m'/weka/s223795137/Crawl_data'\u001b[0m\n",
      "\n",
      "/weka/s223795137/.conda/envs/llava/lib/python3.10/site-packages/langchain/__init__.py:30: UserWarning: Importing LLMChain from langchain root module is no longer supported. Please use langchain.chains.LLMChain instead.\n",
      "  warnings.warn(\n",
      "/weka/s223795137/.conda/envs/llava/lib/python3.10/site-packages/langchain/__init__.py:30: UserWarning: Importing PromptTemplate from langchain root module is no longer supported. Please use langchain_core.prompts.PromptTemplate instead.\n",
      "  warnings.warn(\n",
      "/weka/s223795137/Crawl_data/gradio_run.py:11: LangChainDeprecationWarning: Importing Chroma from langchain.vectorstores is deprecated. Please replace deprecated imports:\n",
      "\n",
      ">> from langchain.vectorstores import Chroma\n",
      "\n",
      "with new imports of:\n",
      "\n",
      ">> from langchain_community.vectorstores import Chroma\n",
      "You can use the langchain cli to **automatically** upgrade many imports. Please see documentation here <https://python.langchain.com/docs/versions/v0_2/>\n",
      "  from langchain.vectorstores import Chroma\n",
      "INFO 03-05 06:54:18 config.py:510] This model supports multiple tasks: {'reward', 'classify', 'score', 'embed', 'generate'}. Defaulting to 'generate'.\n",
      "INFO 03-05 06:54:18 llm_engine.py:234] Initializing an LLM engine (v0.6.6.post1) with config: model='microsoft/phi-4', speculative_config=None, tokenizer='microsoft/phi-4', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=16384, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=microsoft/phi-4, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\"],\"candidate_compile_sizes\":[],\"compile_sizes\":[],\"capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":256}, use_cached_outputs=False, \n",
      "INFO 03-05 06:54:20 selector.py:120] Using Flash Attention backend.\n",
      "INFO 03-05 06:54:20 model_runner.py:1094] Starting to load model microsoft/phi-4...\n",
      "INFO 03-05 06:54:21 weight_utils.py:251] Using model weights format ['*.safetensors']\n",
      "Loading safetensors checkpoint shards:   0% Completed | 0/6 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  17% Completed | 1/6 [00:00<00:03,  1.39it/s]\n",
      "Loading safetensors checkpoint shards:  33% Completed | 2/6 [00:01<00:03,  1.25it/s]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 3/6 [00:02<00:02,  1.04it/s]\n",
      "Loading safetensors checkpoint shards:  67% Completed | 4/6 [00:03<00:02,  1.02s/it]\n",
      "Loading safetensors checkpoint shards:  83% Completed | 5/6 [00:04<00:01,  1.03s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 6/6 [00:05<00:00,  1.02s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 6/6 [00:05<00:00,  1.02it/s]\n",
      "\n",
      "INFO 03-05 06:54:28 model_runner.py:1099] Loading model weights took 27.3875 GB\n",
      "INFO 03-05 06:54:29 worker.py:241] Memory profiling takes 1.30 seconds\n",
      "INFO 03-05 06:54:29 worker.py:241] the current vLLM instance can use total_gpu_memory (79.11GiB) x gpu_memory_utilization (0.90) = 71.20GiB\n",
      "INFO 03-05 06:54:29 worker.py:241] model weights take 27.39GiB; non_torch_memory takes 0.17GiB; PyTorch activation peak memory takes 2.14GiB; the rest of the memory reserved for KV Cache is 41.50GiB.\n",
      "INFO 03-05 06:54:29 gpu_executor.py:76] # GPU blocks: 13598, # CPU blocks: 1310\n",
      "INFO 03-05 06:54:29 gpu_executor.py:80] Maximum concurrency for 16384 tokens per request: 13.28x\n",
      "INFO 03-05 06:54:31 model_runner.py:1415] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "Capturing CUDA graph shapes: 100%|██████████████| 35/35 [00:22<00:00,  1.54it/s]\n",
      "INFO 03-05 06:54:54 model_runner.py:1535] Graph capturing finished in 23 secs, took 1.07 GiB\n",
      "INFO 03-05 06:54:54 llm_engine.py:431] init engine (profile, create kv cache, warmup model) took 25.91 seconds\n",
      "!!!!!!!!!!!!megablocks not available, using torch.matmul instead\n",
      "<All keys matched successfully>\n",
      "/weka/s223795137/Crawl_data/gradio_run.py:187: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-chroma package and should be used instead. To use it run `pip install -U :class:`~langchain-chroma` and import as `from :class:`~langchain_chroma import Chroma``.\n",
      "  vectorstore = Chroma(persist_directory=\"./chroma_db_pages_v3_2.5k\", embedding_function=local_embeddings)\n",
      "/weka/s223795137/Crawl_data/gradio_run.py:230: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
      "  llm_chain = LLMChain(llm=llm, prompt=prompt_template)  # Replace with your LLM\n",
      "/weka/s223795137/Crawl_data/gradio_run.py:233: LangChainDeprecationWarning: This class is deprecated. Use the `create_stuff_documents_chain` constructor instead. See migration guide here: https://python.langchain.com/docs/versions/migrating_chains/stuff_docs_chain/\n",
      "  stuff_chain = StuffDocumentsChain(\n",
      "/weka/s223795137/.conda/envs/llava/lib/python3.10/site-packages/langchain/__init__.py:30: UserWarning: Importing LLMChain from langchain root module is no longer supported. Please use langchain.chains.LLMChain instead.\n",
      "  warnings.warn(\n",
      "/weka/s223795137/.conda/envs/llava/lib/python3.10/site-packages/langchain/__init__.py:30: UserWarning: Importing PromptTemplate from langchain root module is no longer supported. Please use langchain_core.prompts.PromptTemplate instead.\n",
      "  warnings.warn(\n",
      "/weka/s223795137/Crawl_data/gradio_run.py:11: LangChainDeprecationWarning: Importing Chroma from langchain.vectorstores is deprecated. Please replace deprecated imports:\n",
      "\n",
      ">> from langchain.vectorstores import Chroma\n",
      "\n",
      "with new imports of:\n",
      "\n",
      ">> from langchain_community.vectorstores import Chroma\n",
      "You can use the langchain cli to **automatically** upgrade many imports. Please see documentation here <https://python.langchain.com/docs/versions/v0_2/>\n",
      "  from langchain.vectorstores import Chroma\n",
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "* Running on public URL: https://c2fba599f6993b30cb.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n",
      "/weka/s223795137/Crawl_data/gradio_run.py:114: LangChainDeprecationWarning: This class is deprecated. Use the `create_retrieval_chain` constructor instead. See migration guide here: https://python.langchain.com/docs/versions/migrating_chains/retrieval_qa/\n",
      "  rag_chain = RetrievalQA(\n",
      "/weka/s223795137/Crawl_data/gradio_run.py:120: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  response = rag_chain.run(query)\n",
      "Processed prompts: 100%|█| 1/1 [00:03<00:00,  3.33s/it, est. speed input: 608.20\n",
      "Processed prompts: 100%|█| 1/1 [00:04<00:00,  4.08s/it, est. speed input: 688.51\n",
      "Processed prompts: 100%|█| 1/1 [00:02<00:00,  2.23s/it, est. speed input: 1251.2\n",
      "Processed prompts: 100%|█| 1/1 [00:04<00:00,  4.56s/it, est. speed input: 625.60\n",
      "Processed prompts: 100%|█| 1/1 [00:01<00:00,  1.87s/it, est. speed input: 1518.8\n",
      "Processed prompts: 100%|█| 1/1 [00:03<00:00,  3.71s/it, est. speed input: 735.61\n",
      "Processed prompts: 100%|█| 1/1 [00:08<00:00,  8.55s/it, est. speed input: 345.01\n",
      "Processed prompts: 100%|█| 1/1 [00:03<00:00,  3.74s/it, est. speed input: 738.61\n",
      "Processed prompts: 100%|█| 1/1 [00:03<00:00,  3.18s/it, est. speed input: 812.28\n",
      "Processed prompts: 100%|█| 1/1 [00:03<00:00,  3.98s/it, est. speed input: 722.33\n",
      "Keyboard interruption in main thread... closing server.\n",
      "^C\n",
      "[rank0]: Traceback (most recent call last):\n",
      "[rank0]:   File \"/weka/s223795137/.conda/envs/llava/lib/python3.10/site-packages/gradio/blocks.py\", line 2925, in block_thread\n",
      "[rank0]:     time.sleep(0.1)\n",
      "[rank0]: KeyboardInterrupt\n",
      "\n",
      "[rank0]: During handling of the above exception, another exception occurred:\n",
      "\n",
      "[rank0]: Traceback (most recent call last):\n",
      "[rank0]:   File \"/weka/s223795137/Crawl_data/gradio_run.py\", line 255, in <module>\n",
      "[rank0]:     iface.launch(share=True)\n",
      "[rank0]:   File \"/weka/s223795137/.conda/envs/llava/lib/python3.10/site-packages/gradio/blocks.py\", line 2831, in launch\n",
      "[rank0]:     self.block_thread()\n",
      "[rank0]:   File \"/weka/s223795137/.conda/envs/llava/lib/python3.10/site-packages/gradio/blocks.py\", line 2929, in block_thread\n",
      "[rank0]:     self.server.close()\n",
      "[rank0]:   File \"/weka/s223795137/.conda/envs/llava/lib/python3.10/site-packages/gradio/http_server.py\", line 68, in close\n",
      "[rank0]:     self.watch_thread.join()\n",
      "[rank0]:   File \"/weka/s223795137/.conda/envs/llava/lib/python3.10/threading.py\", line 1096, in join\n",
      "[rank0]:     self._wait_for_tstate_lock()\n",
      "[rank0]:   File \"/weka/s223795137/.conda/envs/llava/lib/python3.10/threading.py\", line 1116, in _wait_for_tstate_lock\n",
      "[rank0]:     if lock.acquire(block, timeout):\n",
      "[rank0]: KeyboardInterrupt\n",
      "Killing tunnel 127.0.0.1:7860 <> https://c2fba599f6993b30cb.gradio.live\n"
     ]
    }
   ],
   "source": [
    "!gradio gradio_run.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llava",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

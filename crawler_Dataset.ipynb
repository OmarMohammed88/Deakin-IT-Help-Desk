{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "835a6348-17d1-4770-abdf-7b0162810ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ab63b5d-4a8f-45f3-9285-0541f1d12303",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin, urlparse\n",
    "import time\n",
    "\n",
    "def is_valid(url, base_netloc):\n",
    "    \"\"\"\n",
    "    Check if the URL is valid and belongs to the same domain.\n",
    "    \"\"\"\n",
    "    parsed = urlparse(url)\n",
    "    return parsed.netloc == base_netloc or parsed.netloc == ''\n",
    "\n",
    "def crawl(url, base_netloc, visited, driver):\n",
    "    if url in visited:\n",
    "        return \"\"\n",
    "    visited.add(url)\n",
    "    print(f\"Crawling: {url}\")\n",
    "    text_data = \"\"\n",
    "    \n",
    "    try:\n",
    "        driver.get(url)\n",
    "        time.sleep(4)\n",
    "        page_source = driver.page_source\n",
    "        soup = BeautifulSoup(page_source, 'html.parser')\n",
    "        \n",
    "        for element in soup([\"script\", \"style\"]):\n",
    "            element.decompose()\n",
    "        \n",
    "        page_text = soup.get_text(separator=' ', strip=True)\n",
    "        text_data += page_text + \"\\n\\n\"\n",
    "        print(text_data)\n",
    "        \n",
    "        # Find and crawl all internal links\n",
    "        for link in soup.find_all('a', href=True):\n",
    "            href = link['href']\n",
    "            # Skip invalid or unwanted links\n",
    "            if href.startswith('javascript:') or href.startswith('mailto:') or href.startswith('#'):\n",
    "                continue\n",
    "            next_url = urljoin(url, href)\n",
    "            if is_valid(next_url, base_netloc) and next_url not in visited:\n",
    "                text_data += crawl(next_url, base_netloc, visited, driver)\n",
    "                time.sleep(1)\n",
    "    except Exception as e:\n",
    "        print(f\"Error crawling {url}: {e}\")\n",
    "    return text_data\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument(\"--headless\") \n",
    "\n",
    "    service = Service(\"chromedriver.exe\")  # <-- Update with your chromedriver path\n",
    "    driver = webdriver.Chrome(service=service, options=chrome_options)\n",
    "    \n",
    "    start_url = \"https://help.deakin.edu.au/ithelp?id=it_kb_view\"  \n",
    "    base_netloc = urlparse(start_url).netloc\n",
    "    visited_urls = set()\n",
    "    \n",
    "    all_text = crawl(start_url, base_netloc, visited_urls, driver)\n",
    "    driver.quit()\n",
    "    \n",
    "    with open(\"website_text.txt\", \"w\", encoding=\"utf-8\") as file:\n",
    "        file.write(all_text)\n",
    "    \n",
    "    print(\"Crawling complete. Text saved to website_text.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba38ed02-004c-4274-bbbf-70e0f33eb2f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading\n",
    "import time\n",
    "from queue import Queue\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin, urlparse\n",
    "\n",
    "visited_lock = threading.Lock()\n",
    "visited = set()\n",
    "\n",
    "text_data_lock = threading.Lock()\n",
    "all_text = \"\"\n",
    "\n",
    "url_queue = Queue()\n",
    "\n",
    "def is_valid(url, base_netloc):\n",
    "    \"\"\"\n",
    "    Check if the URL is valid and belongs to the same domain.\n",
    "    \"\"\"\n",
    "    parsed = urlparse(url)\n",
    "    return parsed.netloc == base_netloc or parsed.netloc == ''\n",
    "\n",
    "def worker(base_netloc, driver_path, delay):\n",
    "    \"\"\"\n",
    "    A worker function that creates its own Selenium driver instance, \n",
    "    processes URLs from the queue, extracts text, and enqueues discovered links.\n",
    "    \"\"\"\n",
    "    global all_text\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument(\"--headless\")\n",
    "    service = Service(driver_path)\n",
    "    driver = webdriver.Chrome(service=service, options=chrome_options)\n",
    "    \n",
    "    while True:\n",
    "        try:\n",
    "            current_url = url_queue.get(timeout=5)  \n",
    "        except Exception:\n",
    "            break\n",
    "\n",
    "        with visited_lock:\n",
    "            if current_url in visited:\n",
    "                url_queue.task_done()\n",
    "                continue\n",
    "            visited.add(current_url)\n",
    "\n",
    "        print(f\"Crawling: {current_url}\")\n",
    "        try:\n",
    "            driver.get(current_url)\n",
    "            time.sleep(delay)\n",
    "            page_source = driver.page_source\n",
    "\n",
    "            soup = BeautifulSoup(page_source, 'html.parser')\n",
    "            for element in soup([\"script\", \"style\"]):\n",
    "                element.decompose()\n",
    "            page_text = soup.get_text(separator=' ', strip=True)\n",
    "            with text_data_lock:\n",
    "                all_text += page_text + \"\\n\\n\"\n",
    "                print(all_text)\n",
    "\n",
    "            # Find and queue all valid internal links\n",
    "            for link in soup.find_all('a', href=True):\n",
    "                href = link['href']\n",
    "                if href.startswith('javascript:') or href.startswith('mailto:') or href.startswith('#'):\n",
    "                    continue\n",
    "                next_url = urljoin(current_url, href)\n",
    "                if is_valid(next_url, base_netloc):\n",
    "                    with visited_lock:\n",
    "                        if next_url not in visited:\n",
    "                            url_queue.put(next_url)\n",
    "            time.sleep(delay)\n",
    "        except Exception as e:\n",
    "            print(f\"Error crawling {current_url}: {e}\")\n",
    "        finally:\n",
    "            url_queue.task_done()\n",
    "\n",
    "    driver.quit()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Update these parameters:\n",
    "    start_url = \"https://help.deakin.edu.au/ithelp?id=it_kb_view\"         \n",
    "    driver_path = \"chromedriver.exe\"      # Replace with your chromedriver executable path\n",
    "    num_threads = 4                         \n",
    "    delay = 2                               \n",
    "\n",
    "    base_netloc = urlparse(start_url).netloc\n",
    "    url_queue.put(start_url)\n",
    "\n",
    "    threads = []\n",
    "    for _ in range(num_threads):\n",
    "        t = threading.Thread(target=worker, args=(base_netloc, driver_path, delay))\n",
    "        t.start()\n",
    "        threads.append(t)\n",
    "\n",
    "    url_queue.join()\n",
    "    for t in threads:\n",
    "        t.join()\n",
    "\n",
    "    with open(\"website_text.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(all_text)\n",
    "\n",
    "    print(\"Crawling complete. Text saved to website_text.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2b43556-9b9c-4684-844b-91b51ac430fd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
